{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a3f6a4-8348-4cd6-a336-10ecb9cfaaa3",
   "metadata": {},
   "source": [
    "# Conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f44fbb-6708-4fce-bac4-55dc261e3a2d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7406eb-334b-43d0-8110-31656a55b2fd",
   "metadata": {
    "height": 691
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(\n",
    "        ..., description=\"Latitude of the location to fetch weather data for\"\n",
    "    )\n",
    "    longitude: float = Field(\n",
    "        ..., description=\"Longitude of the location to fetch weather data for\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "\n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\",\n",
    "        \"forecast_days\": 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.now(timezone.utc).replace(tzinfo=None)\n",
    "    time_list = [\n",
    "        datetime.fromisoformat(time_str.replace(\"Z\", \"+00:00\"))\n",
    "        for time_str in results[\"hourly\"][\"time\"]\n",
    "    ]\n",
    "    temperature_list = results[\"hourly\"][\"temperature_2m\"]\n",
    "\n",
    "    closest_time_index = min(\n",
    "        range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time)\n",
    "    )\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "\n",
    "    return f\"The current temperature is {current_temperature}°C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac0a1e7-567f-4775-a538-351a5ff099e8",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[:3]:\n",
    "        try:\n",
    "            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4856d233-0916-4b97-8a7c-b494d5f2b8e0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf0d789-559b-42df-9d6d-77a0cd8888d6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933ce7cb-099c-4f43-a548-b355a5b66511",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xf/7hwvtm5j57s6dtr8_dq1g97m0000gn/T/ipykernel_68131/3768078843.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(temperature=0).bind(functions=functions)\n"
     ]
    }
   ],
   "source": [
    "functions = [convert_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4281a2d8-7884-40e0-b34d-09c39eecbb7a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1dbc697-3e0a-4197-8ddc-2894763b4b85",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab829b59-803f-488f-bcf3-5119c224baa0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b4d483-c498-4932-a373-bc2f4da17c7f",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ac6ea2-0e20-424a-ba33-6f8d627379b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bada1de8-d7c4-4880-98c2-d47cc65f684d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\"input\": \"what is the weather is sf?\", \"agent_scratchpad\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5f4e61-36a3-4d80-9c49-0b83cedb1592",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11845c11-7d41-455a-8ac0-4b8161acaf83",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "observation = get_current_temperature.invoke(result1.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810184c8-8495-4189-b83e-17220cf65bbe",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 14.5°C'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c1b223d-ea2a-414c-924c-512c4c9da66a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentActionMessageLog"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61853679-75b5-4462-8059-f1214286a9eb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72ca4278-165a-4915-8ec6-09e696f29d86",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--89209878-3319-4bc0-9e6e-2383c5e5fdba-0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e079a7-d58c-41c0-8762-4e183e72a1d1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--89209878-3319-4bc0-9e6e-2383c5e5fdba-0'),\n",
       " FunctionMessage(content='The current temperature is 14.5°C', additional_kwargs={}, response_metadata={}, name='get_current_temperature')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_to_openai_functions(\n",
    "    [\n",
    "        (result1, observation),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09735a9e-452d-472b-a4c0-35c315e9e53f",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "result2 = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"what is the weather is sf?\",\n",
    "        \"agent_scratchpad\": format_to_openai_functions([(result1, observation)]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9df61ecc-52fe-4fae-85a2-dbb61d68e2db",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 14.5°C.'}, log='The current temperature in San Francisco is 14.5°C.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9dd139f-e06a-4148-a025-ae100851c63e",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke(\n",
    "            {\n",
    "                \"input\": user_input,\n",
    "                \"agent_scratchpad\": format_to_openai_functions(intermediate_steps),\n",
    "            }\n",
    "        )\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia,\n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f6c1121-70a6-41ae-8c3d-41f7cfc756af",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "agent_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a05df88-4ad9-4c48-a7dd-6b42db91cb33",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke(\n",
    "            {\"input\": user_input, \"intermediate_steps\": intermediate_steps}\n",
    "        )\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia,\n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8bf88b6-9992-4041-8344-ef4921bf3cf2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 14.5°C.'}, log='The current temperature in San Francisco is 14.5°C.')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is the weather is sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c94a493-b480-409a-968a-038c8895709c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for document analysis and summarization, chatbots, and code analysis.'}, log='LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for document analysis and summarization, chatbots, and code analysis.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c8c6fd7-7c8c-487a-81de-1dcff739a2f9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'Well, hello there! How can I assist you today?'}, log='Well, hello there! How can I assist you today?')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b270a064-8cf1-44da-a59e-be622f9099b9",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87baebf4-2555-4e98-aece-a1c327644817",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "Page: Vector database\n",
      "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
      "Vector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\n",
      "Vector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\n",
      "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term RAG was first introduced in a 2020 research paper from Meta.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for document analysis and summarization, chatbots, and code analysis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for document analysis and summarization, chatbots, and code analysis.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2da813c-47c3-487e-8c43-444c9b4d9821",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'output': 'Nice to meet you, Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ba8a9f-fcb2-44d0-878f-3017f607e955",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have access to your personal information. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'output': \"I'm sorry, I don't have access to your personal information. How can I assist you today?\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d1df28a-27da-4f8e-ab03-a37a276db74f",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpful but sassy assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8598e87-de8a-4710-a90e-2a919adc00e2",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "agent_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3865c2c-cb80-4341-bcbe-3b5150749622",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40b036fc-446f-4075-9a92-9a4bac6ecb2b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent_chain, tools=tools, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37f70acf-57ff-4ab9-a942-f0ad6e680c40",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Nice to meet you, Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1554b0c-8c72-4d8d-bbed-20afd94828c5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYou just told me your name is Bob! So, your name is Bob. How can I assist you, Bob?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats my name',\n",
       " 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='whats my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='You just told me your name is Bob! So, your name is Bob. How can I assist you, Bob?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'You just told me your name is Bob! So, your name is Bob. How can I assist you, Bob?'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40887f61-6746-4e51-852d-5ff0457384b7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 14.5°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 14.5°C. Is there anything else you would like to know?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='whats my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='You just told me your name is Bob! So, your name is Bob. How can I assist you, Bob?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='whats the weather in sf?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The current temperature in San Francisco is 14.5°C. Is there anything else you would like to know?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The current temperature in San Francisco is 14.5°C. Is there anything else you would like to know?'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b511-21d6-4c7e-b4e2-f886bc296997",
   "metadata": {},
   "source": [
    "### Create a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1cd4bd2-703e-4e4e-8f5c-ae3d491a41f6",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in\"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67cb6fdc-ff55-4a7b-8a39-1871006aa990",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia, create_your_own]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66a73e69-acfb-471e-b54d-8ac586235521",
   "metadata": {
    "height": 708
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpanel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpn\u001b[39;00m  \u001b[38;5;66;03m# GUI\u001b[39;00m\n\u001b[32m      3\u001b[39m pn.extension()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpanel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'panel'"
     ]
    }
   ],
   "source": [
    "import panel as pn  # GUI\n",
    "\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "\n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__(**params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_to_openai_functions(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True, memory_key=\"chat_history\"\n",
    "        )\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You are helpful but sassy assistant\"),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"user\", \"{input}\"),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                agent_scratchpad=lambda x: format_to_openai_functions(\n",
    "                    x[\"intermediate_steps\"]\n",
    "                )\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.model\n",
    "            | OpenAIFunctionsAgentOutputParser()\n",
    "        )\n",
    "        self.qa = AgentExecutor(\n",
    "            agent=self.chain, tools=tools, verbose=False, memory=self.memory\n",
    "        )\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = \"\"\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result[\"output\"]\n",
    "        self.panels.extend(\n",
    "            [\n",
    "                pn.Row(\"User:\", pn.pane.Markdown(query, width=450)),\n",
    "                pn.Row(\n",
    "                    \"ChatBot:\",\n",
    "                    pn.pane.Markdown(\n",
    "                        self.answer, width=450, styles={\"background-color\": \"#F6F6F6\"}\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "    def clr_history(self, count=0):\n",
    "        self.chat_history = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c3827-87fc-4dbe-a566-ca95b3f9068e",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput(placeholder=\"Enter text here…\")\n",
    "\n",
    "conversation = pn.bind(cb.convchain, inp)\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation, loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown(\"# QnA_Bot\")), pn.Tabs((\"Conversation\", tab1))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41804e2f-4dc1-4763-8893-5e76ab5d8f14",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fd8bc-9da5-4ca0-bfc9-c1bcf579fa05",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b8125-6f4c-47e9-8654-44388b08bd85",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1d6f9-fdc9-42eb-84be-dba81bbb63fd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f302e-fcd3-424b-917a-e0e57602b93f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a43ab-06dc-4b48-b279-72230ab855f8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ea19e-5454-4abe-9b6f-d50c0d594151",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef90005-522a-4497-8247-e46fe0e33b3a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffd8a1-3611-4b98-ad00-8abfdc9400bc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f830c57-669f-4408-bc3d-1b7e309ca414",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2004f20-b480-465d-b881-1776963750fc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfab02a-9c9e-4e9c-adb7-b826c76adc5d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbc6dc-e1da-486f-85d9-17d1ca30edef",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9aed9-9202-4a0c-96fb-15353d4ba84f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e8d1e-359f-484c-8ea0-b53c19d689ec",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c54949-26a0-46f9-b9ab-773ea77c4574",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cfca3-0004-4667-9db4-c1680a32be4d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6178195-8834-48ae-b41c-c1d893027161",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd214a68-f432-4106-aeda-289082ae049e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a4e83-7792-4e4a-8fff-c30dbcaa8be1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b3eed-982f-4e84-8d8d-8c2a7447a717",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337263d-7b23-4584-a9dd-17d89e20e0c3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba85f9-3741-4f37-ba74-28cc59b6acea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a332c84-8019-4211-a61b-f30f4e35d1f7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1622f-156f-4574-909d-0a276e3afe23",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b279336-c369-4b4b-8d0e-dac95590181f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b5c36-fc6b-4f60-b8da-935873302630",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173aaa1-ede3-4c38-ac46-93b9a7352850",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6869c6-ef60-4274-ad18-a8b2b112c205",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-agents-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
