{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/11_bert_pretraining/11_bert_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining a BERT model\n",
        "\n",
        "## Loading data"
      ],
      "metadata": {
        "id": "ht5t3ND0W14q"
      },
      "id": "ht5t3ND0W14q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834d2d6d",
      "metadata": {
        "id": "834d2d6d"
      },
      "outputs": [],
      "source": [
        "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
        "!unzip BERT_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "class BERTCSVDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.data = pd.read_csv(filename)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "\n",
        "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
        "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
        "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
        "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
        "            original_text = row['Original Text']  # If you want to use it\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "            print(\"BERT Input:\", row['BERT Input'])\n",
        "            print(\"BERT Label:\", row['BERT Label'])\n",
        "            # Handle the error, e.g., by skipping this row or using default values\n",
        "            return None  # or some default values\n",
        "\n",
        "            # Tokenizing the original text with BERT\n",
        "        encoded_input = self.tokenizer.encode_plus(\n",
        "            original_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze()\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
        "\n",
        "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
      ],
      "metadata": {
        "id": "1q7jk9mRXCZD"
      },
      "id": "1q7jk9mRXCZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# create a collate function that applies transformations on batches of data iterator\n",
        "PAD_IDX = 0\n",
        "def collate_batch(batch):\n",
        "\n",
        "\n",
        "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
        "\n",
        "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
        "        # Convert each sequence to a tensor and append to the respective list\n",
        "        bert_inputs_batch.append(bert_input.clone().detach())\n",
        "        bert_labels_batch.append(bert_label.clone().detach())\n",
        "        segment_labels_batch.append(segment_label.clone().detach())\n",
        "        is_nexts_batch.append(is_next)\n",
        "        input_ids_batch.append(input_ids)\n",
        "        attention_mask_batch.append(attention_mask)\n",
        "        original_text_battch.append(original_text)\n",
        "\n",
        "    # Pad the sequences in the batch\n",
        "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
      ],
      "metadata": {
        "id": "AwwZtZl9XW15"
      },
      "id": "AwwZtZl9XW15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test dataloaders\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "S_gmGw2fXm3V"
      },
      "id": "S_gmGw2fXm3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation\n",
        "\n",
        "3 types of embeddings used in BERT to represent input tokens:\n",
        "\n",
        "1. Token Embedding: initial representation of each token\n",
        "2. Positional Embedding: captures the order of tokens\n",
        "3. Segment Embedding: differentiates between different segments (e.g. sentences)\n",
        "\n",
        "Model components:\n",
        "\n",
        "1. Initialization: subclass of `torch.nn.Module`\n",
        "2. Embedding Layer: combines token embeddings and segment embeddings\n",
        "3. Transformer Encoder: encodes the input embeddings\n",
        "4. Next Sentence Prediction: predicts the relationship between two consecutive sentences using the output of Transformer encoder\n",
        "5. Masked Language Modeling: predicts the masked tokens in the input sequence\n",
        "6. Forward Pass: defines the forward pass. Returns predictions for Next Sentence Prediction and Masked Language Modeling using input tokens and segment labels\n"
      ],
      "metadata": {
        "id": "bbB6PlgrXzro"
      },
      "id": "bbB6PlgrXzro"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        # Apply the positional encodings to the input token embeddings\n",
        "\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class BERTEmbedding (nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
        "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
        "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels=False):\n",
        "        my_embeddings=self.token_embedding(bert_inputs)\n",
        "        if self.train:\n",
        "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
        "        else:\n",
        "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "o9HyfMDHXtQq"
      },
      "id": "o9HyfMDHXtQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "VOCAB_SIZE=147161\n",
        "batch = 2\n",
        "count = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# load sample batches from dataloader\n",
        "for batch in train_dataloader:\n",
        "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break\n",
        "\n",
        "print(bert_inputs.shape)\n",
        "print(bert_inputs[:,0])\n",
        "print(segment_labels.shape)\n",
        "print(segment_labels[:,0])"
      ],
      "metadata": {
        "id": "EK5F-9sbYJsd"
      },
      "id": "EK5F-9sbYJsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the TokenEmbedding\n",
        "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM )\n",
        "\n",
        "# Get the token embeddings for a sample input\n",
        "t_embeddings = token_embedding(bert_inputs)\n",
        "# Each token is transformed into a tensor of size emb_size\n",
        "print(f\"Dimensions of token embeddings: {t_embeddings.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
        "for i in range(3):\n",
        "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
      ],
      "metadata": {
        "id": "M7-AUZWEYpLP"
      },
      "id": "M7-AUZWEYpLP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0)\n",
        "\n",
        "# Apply positional encoding to token embeddings\n",
        "p_embedding = positional_encoding(t_embeddings)\n",
        "\n",
        "print(f\"Dimensions of positionally encoded tokens: {p_embedding.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the positional encoded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get encoded_tokens[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Positional Embeddings for the {i}th token of the first sample: {p_embedding[i,0,:]}\")"
      ],
      "metadata": {
        "id": "opNSdoOha_Ci"
      },
      "id": "opNSdoOha_Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_embedding = nn.Embedding(3, EMBEDDING_DIM)\n",
        "s_embedding = segment_embedding(segment_labels)\n",
        "print(f\"Dimensions of segment embedding: {s_embedding.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the Segment Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get segment_embedded[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Segment Embeddings for the {i}th token of the first sample: {s_embedding[i,0,:]}\")"
      ],
      "metadata": {
        "id": "7ZeEI7ekYvA3"
      },
      "id": "7ZeEI7ekYvA3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the combined embedding vectors\n",
        "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
        "print(f\"Dimensions of token + position + segment encoded tokens: {bert_embeddings.size()}\")\n",
        "#Check the BERT Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get bert_embeddings[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"BERT_Embedding for {i}th token: {bert_embeddings[i,0,:]}\")"
      ],
      "metadata": {
        "id": "pGiHwdOIaog0"
      },
      "id": "pGiHwdOIaog0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: The size of the vocabulary.\n",
        "        d_model: The size of the embeddings (hidden size).\n",
        "        n_layers: The number of Transformer layers.\n",
        "        heads: The number of attention heads in each Transformer layer.\n",
        "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # Embedding layer that combines token embeddings and segment embeddings\n",
        "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Linear layer for Next Sentence Prediction\n",
        "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "\n",
        "        # Linear layer for Masked Language Modeling\n",
        "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels):\n",
        "        \"\"\"\n",
        "        bert_inputs: Input tokens.\n",
        "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
        "        mask: Attention mask to prevent attention to padding tokens.\n",
        "\n",
        "        return: Predictions for next sentence task and masked language modeling task.\n",
        "        \"\"\"\n",
        "\n",
        "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "        # Generate embeddings from input tokens and segment labels\n",
        "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
        "\n",
        "        # Pass embeddings through the Transformer encoder\n",
        "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "\n",
        "\n",
        "        # Masked Language Modeling: Predict all tokens in the sequence\n",
        "        masked_language = self.masked_language(transformer_encoder_output)\n",
        "\n",
        "        return  next_sentence_prediction, masked_language"
      ],
      "metadata": {
        "id": "3pJeJYUFa5OT"
      },
      "id": "3pJeJYUFa5OT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the model\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Define parameters\n",
        "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
        "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
        "n_layers = 2  # Number of Transformer layers\n",
        "initial_heads = 12 # Initial number of attention heads\n",
        "initial_heads = 2\n",
        "# Ensure the number of heads is a factor of the embedding dimension\n",
        "heads = initial_heads - d_model % initial_heads\n",
        "\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Create an instance of the BERT model\n",
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
      ],
      "metadata": {
        "id": "z5a9nr0RbUhU"
      },
      "id": "z5a9nr0RbUhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "padding_mask.shape"
      ],
      "metadata": {
        "id": "AVuV41nibbkO"
      },
      "id": "AVuV41nibbkO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "# Pass embeddings through the Transformer encoder\n",
        "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
        "transformer_encoder_output.shape"
      ],
      "metadata": {
        "id": "xM_tp6iBblH1"
      },
      "id": "xM_tp6iBblH1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "#logits for NSP task\n",
        "print(f\"NSP Output Shape: {nsp.shape}\")  # Expected shape: (batch_size, 2)"
      ],
      "metadata": {
        "id": "DsTNyIqObnch"
      },
      "id": "DsTNyIqObnch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_language = nn.Linear(d_model, vocab_size)\n",
        "# Masked Language Modeling: Predict all tokens in the sequence\n",
        "mlm = masked_language(transformer_encoder_output)\n",
        "#logits for MLM task\n",
        "print(f\"MLM Output Shape: {mlm.shape}\")  # Expected shape: (seq_length, batch_size, vocab_size)"
      ],
      "metadata": {
        "id": "Peocm1oqbvQs"
      },
      "id": "Peocm1oqbvQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "gR06nPH8dJHt"
      },
      "id": "gR06nPH8dJHt"
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX=0\n",
        "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
        "loss_fn_nsp = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "device\n",
        "\n",
        "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
        "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
        "\n",
        "    total_loss = 0\n",
        "    total_next_sentence_loss = 0\n",
        "    total_mask_loss = 0\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        for batch in dataloader:\n",
        "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "            # Calculate loss for next sentence prediction\n",
        "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
        "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
        "\n",
        "            # Calculate loss for predicting masked tokens\n",
        "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
        "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "            # Sum up the two losses\n",
        "            loss = next_loss + mask_loss\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            else:\n",
        "                total_loss += loss.item()\n",
        "                total_next_sentence_loss += next_loss.item()\n",
        "                total_mask_loss += mask_loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / (total_batches + 1)\n",
        "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
        "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "IGlFQ3gibyHg"
      },
      "id": "IGlFQ3gibyHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "(Use randomly sampled dataset to reduce processing time.)"
      ],
      "metadata": {
        "id": "Jc6-my1Mdmh7"
      },
      "id": "Jc6-my1Mdmh7"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 3\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data_sampled.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data_sampled.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "_Op7m_rmdj3A"
      },
      "id": "_Op7m_rmdj3A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "# Training loop setup\n",
        "num_epochs = 1\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "# Define the number of warmup steps, e.g., 10% of total\n",
        "warmup_steps = int(total_steps * 0.1)\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# Lists to store losses for plotting\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
        "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "        loss = next_loss + mask_loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "        else:\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader) + 1\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
        "    eval_losses.append(eval_loss)"
      ],
      "metadata": {
        "id": "3hlCAVM_d6Ji"
      },
      "id": "3hlCAVM_d6Ji",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the loss values\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
        "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Evaluation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ju5Un4f-d7jC"
      },
      "id": "Ju5Un4f-d7jC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "(load model from pt file)"
      ],
      "metadata": {
        "id": "sA5hEd34e7bo"
      },
      "id": "sA5hEd34e7bo"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout)  # Ensure these parameters match the original model's\n",
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt'\n",
        "model.load_state_dict(torch.load('H04Cs7O75aOfmJ4YP2HdPw.pt',map_location=torch.device('cpu')))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "O3yvB2Hcey5n"
      },
      "id": "O3yvB2Hcey5n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer with the BERT model's vocabulary\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
        "    # Tokenize sentences with special tokens\n",
        "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
        "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
        "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        # Assuming the model returns NSP predictions first\n",
        "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
        "        # Select the first element (first sequence) of the logits tensor\n",
        "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
        "        logits = torch.softmax(first_logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Interpret the prediction\n",
        "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"The S&P dropped 10% after the Iraqi war.\"\n",
        "sentence2 = \"I hate chocolate\"\n",
        "\n",
        "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
      ],
      "metadata": {
        "id": "SwjBcE8IfCuR"
      },
      "id": "SwjBcE8IfCuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mlm(sentence, model, tokenizer):\n",
        "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    tokens_tensor = inputs.input_ids\n",
        "\n",
        "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
        "    segment_labels = torch.zeros_like(tokens_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model, now correctly handling the output tuple\n",
        "        output_tuple = model(tokens_tensor, segment_labels)\n",
        "\n",
        "        # Assuming the second element of the tuple contains the MLM logits\n",
        "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
        "\n",
        "        # Identify the position of the [MASK] token\n",
        "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Get the predicted index for the [MASK] token from the MLM logits\n",
        "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
        "\n",
        "        # Replace [MASK] in the original sentence with the predicted token\n",
        "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The cat sat on the [MASK].\"\n",
        "print(predict_mlm(sentence, model, tokenizer))"
      ],
      "metadata": {
        "id": "epq1R2evfHEC"
      },
      "id": "epq1R2evfHEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Next Sentence Prediction (NSP) with BERT"
      ],
      "metadata": {
        "id": "8QTXrY1Zf0IN"
      },
      "id": "8QTXrY1Zf0IN"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForPreTraining\n",
        "\n",
        "# Load pretrained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pretrained model (weights)\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "# Prepare text pair for NSP\n",
        "text_1 = \"The cat sat on the mat\"\n",
        "text_2 = \"It was a sunny day\"\n",
        "# Encode text\n",
        "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
        "    nsp_logits = outputs.seq_relationship_logits\n",
        "\n",
        "# Interpret the result for NSP\n",
        "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
        "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
        "else:\n",
        "    print(\"The model thinks these sentences are consecutive.\")"
      ],
      "metadata": {
        "id": "vX7AaMvvfYGk"
      },
      "id": "vX7AaMvvfYGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Masked Language Modeling (MLM) with BERT"
      ],
      "metadata": {
        "id": "DQ0F5aTZgX6J"
      },
      "id": "DQ0F5aTZgX6J"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4FlvdA3gPjr"
      },
      "id": "B4FlvdA3gPjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QapPgXpgvvN"
      },
      "id": "_QapPgXpgvvN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}