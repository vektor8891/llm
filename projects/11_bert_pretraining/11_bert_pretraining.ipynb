{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/11_bert_pretraining/11_bert_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining a BERT model\n",
        "\n",
        "## Loading data"
      ],
      "metadata": {
        "id": "ht5t3ND0W14q"
      },
      "id": "ht5t3ND0W14q"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "834d2d6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834d2d6d",
        "outputId": "bcdd19b5-c101-4e6f-86e7-66da7406985f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-24 15:13:19--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88958506 (85M) [application/zip]\n",
            "Saving to: ‘BERT_dataset.zip’\n",
            "\n",
            "BERT_dataset.zip    100%[===================>]  84.84M  23.7MB/s    in 3.9s    \n",
            "\n",
            "2025-04-24 15:13:23 (22.0 MB/s) - ‘BERT_dataset.zip’ saved [88958506/88958506]\n",
            "\n",
            "Archive:  BERT_dataset.zip\n",
            "   creating: bert_dataset/\n",
            "  inflating: bert_dataset/.DS_Store  \n",
            "  inflating: bert_dataset/bert_train_data.csv  \n",
            "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
            "  inflating: bert_dataset/bert_test_data.csv  \n",
            "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
        "!unzip BERT_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "class BERTCSVDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.data = pd.read_csv(filename)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "\n",
        "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
        "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
        "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
        "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
        "            original_text = row['Original Text']  # If you want to use it\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "            print(\"BERT Input:\", row['BERT Input'])\n",
        "            print(\"BERT Label:\", row['BERT Label'])\n",
        "            # Handle the error, e.g., by skipping this row or using default values\n",
        "            return None  # or some default values\n",
        "\n",
        "            # Tokenizing the original text with BERT\n",
        "        encoded_input = self.tokenizer.encode_plus(\n",
        "            original_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze()\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
        "\n",
        "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
      ],
      "metadata": {
        "id": "1q7jk9mRXCZD"
      },
      "id": "1q7jk9mRXCZD",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# create a collate function that applies transformations on batches of data iterator\n",
        "PAD_IDX = 0\n",
        "def collate_batch(batch):\n",
        "\n",
        "\n",
        "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
        "\n",
        "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
        "        # Convert each sequence to a tensor and append to the respective list\n",
        "        bert_inputs_batch.append(bert_input.clone().detach())\n",
        "        bert_labels_batch.append(bert_label.clone().detach())\n",
        "        segment_labels_batch.append(segment_label.clone().detach())\n",
        "        is_nexts_batch.append(is_next)\n",
        "        input_ids_batch.append(input_ids)\n",
        "        attention_mask_batch.append(attention_mask)\n",
        "        original_text_battch.append(original_text)\n",
        "\n",
        "    # Pad the sequences in the batch\n",
        "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
      ],
      "metadata": {
        "id": "AwwZtZl9XW15"
      },
      "id": "AwwZtZl9XW15",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test dataloaders\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "S_gmGw2fXm3V"
      },
      "id": "S_gmGw2fXm3V",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation"
      ],
      "metadata": {
        "id": "bbB6PlgrXzro"
      },
      "id": "bbB6PlgrXzro"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        # Apply the positional encodings to the input token embeddings\n",
        "\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class BERTEmbedding (nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
        "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
        "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels=False):\n",
        "        my_embeddings=self.token_embedding(bert_inputs)\n",
        "        if self.train:\n",
        "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
        "        else:\n",
        "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "o9HyfMDHXtQq"
      },
      "id": "o9HyfMDHXtQq",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "VOCAB_SIZE=147161\n",
        "batch = 2\n",
        "count = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# load sample batches from dataloader\n",
        "for batch in train_dataloader:\n",
        "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break\n",
        "\n",
        "print(bert_inputs.shape)\n",
        "print(bert_inputs[:,0])"
      ],
      "metadata": {
        "id": "EK5F-9sbYJsd",
        "outputId": "ba79aefc-e3e5-4740-f6ec-49555bd09211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EK5F-9sbYJsd",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 2])\n",
            "tensor([   1,   44,    3,    3,   10,    9,   26,    3,    3,   89, 5550,   11,\n",
            "          40,   45,    3, 1620,    6,    2,  199,    7,   16, 1525,  506,  184,\n",
            "         901,    3,   17,  316,   26,    3,    2,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7-AUZWEYpLP",
        "outputId": "46996bb3-e3dd-4bcd-f86c-a7afd6021d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M7-AUZWEYpLP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([106, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZeEI7ekYvA3"
      },
      "id": "7ZeEI7ekYvA3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}