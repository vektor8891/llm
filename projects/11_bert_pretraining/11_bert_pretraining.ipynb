{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/11_bert_pretraining/11_bert_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["# Pretraining a BERT model\n", "\n", "## Loading data"],
      "metadata": {
        "id": "ht5t3ND0W14q"
      },
      "id": "ht5t3ND0W14q"
    },
    {
      "cell_type": "code",
      "source": ["# !pip install huggingface_hub[hf_xet]"],
      "metadata": {
        "id": "eMChUcGZ7RWv"
      },
      "id": "eMChUcGZ7RWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "834d2d6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834d2d6d",
        "outputId": "a655d10c-5670-47f6-a0c2-69d57b0bd908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-25 12:27:56--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88958506 (85M) [application/zip]\n",
            "Saving to: \u2018BERT_dataset.zip\u2019\n",
            "\n",
            "BERT_dataset.zip    100%[===================>]  84.84M  32.8MB/s    in 2.6s    \n",
            "\n",
            "2025-04-25 12:28:00 (32.8 MB/s) - \u2018BERT_dataset.zip\u2019 saved [88958506/88958506]\n",
            "\n",
            "Archive:  BERT_dataset.zip\n",
            "   creating: bert_dataset/\n",
            "  inflating: bert_dataset/.DS_Store  \n",
            "  inflating: bert_dataset/bert_train_data.csv  \n",
            "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
            "  inflating: bert_dataset/bert_test_data.csv  \n",
            "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
        "!unzip BERT_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "class BERTCSVDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.data = pd.read_csv(filename)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "\n",
        "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
        "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
        "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
        "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
        "            original_text = row['Original Text']  # If you want to use it\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "            print(\"BERT Input:\", row['BERT Input'])\n",
        "            print(\"BERT Label:\", row['BERT Label'])\n",
        "            # Handle the error, e.g., by skipping this row or using default values\n",
        "            return None  # or some default values\n",
        "\n",
        "            # Tokenizing the original text with BERT\n",
        "        encoded_input = self.tokenizer.encode_plus(\n",
        "            original_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze()\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
        "\n",
        "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
      ],
      "metadata": {
        "id": "1q7jk9mRXCZD"
      },
      "id": "1q7jk9mRXCZD",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# create a collate function that applies transformations on batches of data iterator\n",
        "PAD_IDX = 0\n",
        "def collate_batch(batch):\n",
        "\n",
        "\n",
        "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
        "\n",
        "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
        "        # Convert each sequence to a tensor and append to the respective list\n",
        "        bert_inputs_batch.append(bert_input.clone().detach())\n",
        "        bert_labels_batch.append(bert_label.clone().detach())\n",
        "        segment_labels_batch.append(segment_label.clone().detach())\n",
        "        is_nexts_batch.append(is_next)\n",
        "        input_ids_batch.append(input_ids)\n",
        "        attention_mask_batch.append(attention_mask)\n",
        "        original_text_battch.append(original_text)\n",
        "\n",
        "    # Pad the sequences in the batch\n",
        "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
      ],
      "metadata": {
        "id": "AwwZtZl9XW15"
      },
      "id": "AwwZtZl9XW15",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test dataloaders\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "0d3799c8fca44473ae6ce2490379bdd9",
            "28f538a14c4e47eab992351abd866ca2",
            "5daad4371b6e45a88bc2750c6a3a42ae",
            "f9ed09a9eb2d4782adab4f274a547ab5",
            "931271c9d4a84459bc0382e08088d5fb",
            "c5e6e0f5cb09435b85a2dcd6267e17b8",
            "37f7c496083f4d98be8ccd8ce16394e1",
            "0eac3ed46fe34e9c9ecf9aecff1e1657",
            "d1c141a3b35140789bd48105885bec47",
            "6de02b0f21a741d1bdcc707ef1d8fa37",
            "2682f39fcd86458995b3e2ca4e878771",
            "d33659f1a84646faa5953491de99e9bc",
            "d5cd25927fa54952adddc54fe50a0e38",
            "d384f265cedc4176b057fed443edf276",
            "e315d23a8572461f95b2c10fb5b47244",
            "ded4b9ad5bce4cea865450ca19335099",
            "d19be670cc2d4c899a0332fb8e3b43ea",
            "8b1e133b7d324730b62f5d1276e83f7b",
            "49d696526cc7435b86c72d183bffaffd",
            "dc256742b1ed4dd1bcb95dc5e8682c02",
            "19492ec9d81248d5b8b6099aecd85159",
            "9910251eef4b4870aa616b29470b48d2",
            "5a6b6e7c12f14b8e873567f74a3f05de",
            "7b98fd776cfe49ab970d7880057ef0e9",
            "5b383a7caa414662ace6cc3a6e68ef84",
            "9f45f29fde0243e697a31072a4f26192",
            "3ef493c235364c25be959c5845dcaeb8",
            "059484ee51e94db5a1ab9b14f6c308bb",
            "f610f2f1e8d04e8f882b4720698f6e7c",
            "a89425b1fde8416ea70e9a6c6ff881f7",
            "87d738c9572c46eca7c2bfeb4e7570d4",
            "b0eafe88c85c421ca73652c3e7c4d4c8",
            "7d09f643bcb24da3b33af8b9310b9d6c",
            "b5404705adce46c3b7473a1ce62e48e2",
            "828064e6b0824b80a718c29343e6d8fd",
            "1a2c2875aec84a809b063d8d9e6f53ec",
            "fdc287414de645efb89c75f908e32d1e",
            "5f221d2f3ab54fe5afa56336c16e25a1",
            "22af63e8885f47f2be7f087c5cbc42ad",
            "8dd7198e23b746e9a9bb413f29f435b9",
            "99df3098284148d0a63ad3cf2c2ffa35",
            "6f26f3f932294bdaa8734c728102f28f",
            "d8b90892508d49b58f9199f93397eabd",
            "add475211df34fdb9b5338e4543aefc7"
          ]
        },
        "id": "S_gmGw2fXm3V",
        "outputId": "cf7437f1-9606-4fab-9f43-4b25e679974c"
      },
      "id": "S_gmGw2fXm3V",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d3799c8fca44473ae6ce2490379bdd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33659f1a84646faa5953491de99e9bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a6b6e7c12f14b8e873567f74a3f05de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5404705adce46c3b7473a1ce62e48e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation\n",
        "\n",
        "3 types of embeddings used in BERT to represent input tokens:\n",
        "\n",
        "1. Token Embedding: initial representation of each token\n",
        "2. Positional Embedding: captures the order of tokens\n",
        "3. Segment Embedding: differentiates between different segments (e.g. sentences)\n",
        "\n",
        "Model components:\n",
        "\n",
        "1. Initialization: subclass of `torch.nn.Module`\n",
        "2. Embedding Layer: combines token embeddings and segment embeddings\n",
        "3. Transformer Encoder: encodes the input embeddings\n",
        "4. Next Sentence Prediction: predicts the relationship between two consecutive sentences using the output of Transformer encoder\n",
        "5. Masked Language Modeling: predicts the masked tokens in the input sequence\n",
        "6. Forward Pass: defines the forward pass. Returns predictions for Next Sentence Prediction and Masked Language Modeling using input tokens and segment labels\n"
      ],
      "metadata": {
        "id": "bbB6PlgrXzro"
      },
      "id": "bbB6PlgrXzro"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        # Apply the positional encodings to the input token embeddings\n",
        "\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class BERTEmbedding (nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
        "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
        "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels=False):\n",
        "        my_embeddings=self.token_embedding(bert_inputs)\n",
        "        if self.train:\n",
        "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
        "        else:\n",
        "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "o9HyfMDHXtQq"
      },
      "id": "o9HyfMDHXtQq",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "VOCAB_SIZE=147161\n",
        "batch = 2\n",
        "count = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# load sample batches from dataloader\n",
        "for batch in train_dataloader:\n",
        "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break\n",
        "\n",
        "print(bert_inputs.shape)\n",
        "print(bert_inputs[:,0])\n",
        "print(segment_labels.shape)\n",
        "print(segment_labels[:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK5F-9sbYJsd",
        "outputId": "ae0eba91-9484-4281-b2dd-adab4d2e9669"
      },
      "id": "EK5F-9sbYJsd",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([58, 2])\n",
            "tensor([    1,    18,    12,     3,   425,    11,    37,   709,    58,     2,\n",
            "            0,     0,     0,    57,   564,    63,    29,   205,     9,  3270,\n",
            "          138, 12173,     7,     3,    58,     2,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "torch.Size([58, 2])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the TokenEmbedding\n",
        "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM )\n",
        "\n",
        "# Get the token embeddings for a sample input\n",
        "t_embeddings = token_embedding(bert_inputs)\n",
        "# Each token is transformed into a tensor of size emb_size\n",
        "print(f\"Dimensions of token embeddings: {t_embeddings.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
        "for i in range(3):\n",
        "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7-AUZWEYpLP",
        "outputId": "1f086062-8d84-4910-e8ee-5550c8913df3"
      },
      "id": "M7-AUZWEYpLP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of token embeddings: torch.Size([58, 2, 10])\n",
            "Token Embeddings for the 0th token of the first sample: tensor([-2.0008, -3.8798,  6.5487,  0.5648,  0.8633, -2.9579,  5.6600, -4.0631,\n",
            "         2.4579, -0.9972], grad_fn=<SliceBackward0>)\n",
            "Token Embeddings for the 1th token of the first sample: tensor([-0.1088, -1.3450, -0.6220, -0.0096, -2.9501, -2.8600, -2.1335, -7.2497,\n",
            "        -3.1335,  0.6685], grad_fn=<SliceBackward0>)\n",
            "Token Embeddings for the 2th token of the first sample: tensor([ 4.8187, -0.1377,  1.5525, -2.5659, -2.7598,  1.9919,  1.5602, -0.5791,\n",
            "        -4.5657, -7.0449], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0)\n",
        "\n",
        "# Apply positional encoding to token embeddings\n",
        "p_embedding = positional_encoding(t_embeddings)\n",
        "\n",
        "print(f\"Dimensions of positionally encoded tokens: {p_embedding.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the positional encoded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get encoded_tokens[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Positional Embeddings for the {i}th token of the first sample: {p_embedding[i,0,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opNSdoOha_Ci",
        "outputId": "4c2034fa-5829-4535-b059-cfe318f51625"
      },
      "id": "opNSdoOha_Ci",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of positionally encoded tokens: torch.Size([58, 2, 10])\n",
            "Positional Embeddings for the 0th token of the first sample: tensor([-2.0008e+00, -2.8798e+00,  6.5487e+00,  1.5648e+00,  8.6328e-01,\n",
            "        -1.9579e+00,  5.6600e+00, -3.0631e+00,  2.4579e+00,  2.7899e-03],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Positional Embeddings for the 1th token of the first sample: tensor([ 0.7327, -0.8047, -0.4642,  0.9779, -2.9250, -1.8603, -2.1295, -6.2498,\n",
            "        -3.1329,  1.6685], grad_fn=<SliceBackward0>)\n",
            "Positional Embeddings for the 2th token of the first sample: tensor([ 5.7280, -0.5539,  1.8642, -1.6158, -2.7096,  2.9907,  1.5682,  0.4208,\n",
            "        -4.5644, -6.0449], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment_embedding = nn.Embedding(3, EMBEDDING_DIM)\n",
        "s_embedding = segment_embedding(segment_labels)\n",
        "print(f\"Dimensions of segment embedding: {s_embedding.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "# Check the Segment Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get segment_embedded[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Segment Embeddings for the {i}th token of the first sample: {s_embedding[i,0,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZeEI7ekYvA3",
        "outputId": "fcc169fe-5665-4126-de46-53f58e6a6333"
      },
      "id": "7ZeEI7ekYvA3",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of segment embedding: torch.Size([58, 2, 10])\n",
            "Segment Embeddings for the 0th token of the first sample: tensor([-1.1096,  1.4824, -0.0884, -1.5296, -1.6593, -1.0829,  0.1962,  0.1979,\n",
            "         0.5246, -0.0645], grad_fn=<SliceBackward0>)\n",
            "Segment Embeddings for the 1th token of the first sample: tensor([-1.1096,  1.4824, -0.0884, -1.5296, -1.6593, -1.0829,  0.1962,  0.1979,\n",
            "         0.5246, -0.0645], grad_fn=<SliceBackward0>)\n",
            "Segment Embeddings for the 2th token of the first sample: tensor([-1.1096,  1.4824, -0.0884, -1.5296, -1.6593, -1.0829,  0.1962,  0.1979,\n",
            "         0.5246, -0.0645], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the combined embedding vectors\n",
        "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
        "print(f\"Dimensions of token + position + segment encoded tokens: {bert_embeddings.size()}\")\n",
        "#Check the BERT Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get bert_embeddings[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"BERT_Embedding for {i}th token: {bert_embeddings[i,0,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGiHwdOIaog0",
        "outputId": "bb9c9ba5-0526-4deb-a296-a10d1f28ee72"
      },
      "id": "pGiHwdOIaog0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of token + position + segment encoded tokens: torch.Size([58, 2, 10])\n",
            "BERT_Embedding for 0th token: tensor([-5.1111, -5.2772, 13.0091,  0.6000,  0.0673, -5.9986, 11.5161, -6.9284,\n",
            "         5.4405, -1.0590], grad_fn=<SliceBackward0>)\n",
            "BERT_Embedding for 1th token: tensor([ -0.4857,  -0.6672,  -1.1746,  -0.5613,  -7.5343,  -5.8032,  -4.0669,\n",
            "        -13.3016,  -5.7418,   2.2724], grad_fn=<SliceBackward0>)\n",
            "BERT_Embedding for 2th token: tensor([  9.4372,   0.7908,   3.3283,  -5.7113,  -7.1287,   3.8997,   3.3245,\n",
            "          0.0396,  -8.6054, -13.1543], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: The size of the vocabulary.\n",
        "        d_model: The size of the embeddings (hidden size).\n",
        "        n_layers: The number of Transformer layers.\n",
        "        heads: The number of attention heads in each Transformer layer.\n",
        "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # Embedding layer that combines token embeddings and segment embeddings\n",
        "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Linear layer for Next Sentence Prediction\n",
        "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "\n",
        "        # Linear layer for Masked Language Modeling\n",
        "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels):\n",
        "        \"\"\"\n",
        "        bert_inputs: Input tokens.\n",
        "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
        "        mask: Attention mask to prevent attention to padding tokens.\n",
        "\n",
        "        return: Predictions for next sentence task and masked language modeling task.\n",
        "        \"\"\"\n",
        "\n",
        "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "        # Generate embeddings from input tokens and segment labels\n",
        "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
        "\n",
        "        # Pass embeddings through the Transformer encoder\n",
        "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "\n",
        "\n",
        "        # Masked Language Modeling: Predict all tokens in the sequence\n",
        "        masked_language = self.masked_language(transformer_encoder_output)\n",
        "\n",
        "        return  next_sentence_prediction, masked_language"
      ],
      "metadata": {
        "id": "3pJeJYUFa5OT"
      },
      "id": "3pJeJYUFa5OT",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the model\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Define parameters\n",
        "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
        "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
        "n_layers = 2  # Number of Transformer layers\n",
        "initial_heads = 12 # Initial number of attention heads\n",
        "initial_heads = 2\n",
        "# Ensure the number of heads is a factor of the embedding dimension\n",
        "heads = initial_heads - d_model % initial_heads\n",
        "\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Create an instance of the BERT model\n",
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5a9nr0RbUhU",
        "outputId": "aea2259d-2aa8-4102-ea65-3f74960d8327"
      },
      "id": "z5a9nr0RbUhU",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "padding_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVuV41nibbkO",
        "outputId": "4899b386-aef6-4d1f-a211-5a74f21d73a9"
      },
      "id": "AVuV41nibbkO",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ["torch.Size([2, 58])"]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "# Pass embeddings through the Transformer encoder\n",
        "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
        "transformer_encoder_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM_tp6iBblH1",
        "outputId": "119eecd0-a568-4709-a5d6-9294c1781f41"
      },
      "id": "xM_tp6iBblH1",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ["torch.Size([58, 2, 10])"]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "#logits for NSP task\n",
        "print(f\"NSP Output Shape: {nsp.shape}\")  # Expected shape: (batch_size, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsTNyIqObnch",
        "outputId": "ab1d5804-3605-407d-b4b5-6135a53993b4"
      },
      "id": "DsTNyIqObnch",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["NSP Output Shape: torch.Size([2, 2])\n"]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_language = nn.Linear(d_model, vocab_size)\n",
        "# Masked Language Modeling: Predict all tokens in the sequence\n",
        "mlm = masked_language(transformer_encoder_output)\n",
        "#logits for MLM task\n",
        "print(f\"MLM Output Shape: {mlm.shape}\")  # Expected shape: (seq_length, batch_size, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peocm1oqbvQs",
        "outputId": "5d436092-2ea8-45e6-fb9f-954016daadfa"
      },
      "id": "Peocm1oqbvQs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["MLM Output Shape: torch.Size([58, 2, 147161])\n"]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["## Evaluation"],
      "metadata": {
        "id": "gR06nPH8dJHt"
      },
      "id": "gR06nPH8dJHt"
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX=0\n",
        "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
        "loss_fn_nsp = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "device\n",
        "\n",
        "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
        "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
        "\n",
        "    total_loss = 0\n",
        "    total_next_sentence_loss = 0\n",
        "    total_mask_loss = 0\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        for batch in dataloader:\n",
        "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "            # Calculate loss for next sentence prediction\n",
        "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
        "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
        "\n",
        "            # Calculate loss for predicting masked tokens\n",
        "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
        "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "            # Sum up the two losses\n",
        "            loss = next_loss + mask_loss\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            else:\n",
        "                total_loss += loss.item()\n",
        "                total_next_sentence_loss += next_loss.item()\n",
        "                total_mask_loss += mask_loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / (total_batches + 1)\n",
        "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
        "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "IGlFQ3gibyHg"
      },
      "id": "IGlFQ3gibyHg",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "(Use randomly sampled dataset to reduce processing time.)"
      ],
      "metadata": {
        "id": "Jc6-my1Mdmh7"
      },
      "id": "Jc6-my1Mdmh7"
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 3\n",
        "\n",
        "# train_dataset_path = './bert_dataset/bert_train_data_sampled.csv'\n",
        "# test_dataset_path = './bert_dataset/bert_test_data_sampled.csv'\n",
        "\n",
        "# train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "# test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "_Op7m_rmdj3A"
      },
      "id": "_Op7m_rmdj3A",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import Adam\n",
        "# from transformers import get_linear_schedule_with_warmup\n",
        "# from tqdm import tqdm\n",
        "\n",
        "\n",
        "# # Define the optimizer\n",
        "# optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "# # Training loop setup\n",
        "# num_epochs = 1\n",
        "# total_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "# # Define the number of warmup steps, e.g., 10% of total\n",
        "# warmup_steps = int(total_steps * 0.1)\n",
        "\n",
        "# # Create the learning rate scheduler\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "#                                             num_warmup_steps=warmup_steps,\n",
        "#                                             num_training_steps=total_steps)\n",
        "\n",
        "# # Lists to store losses for plotting\n",
        "# train_losses = []\n",
        "# eval_losses = []\n",
        "\n",
        "# for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
        "#         bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "#         next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
        "#         mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "#         loss = next_loss + mask_loss\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()  # Update the learning rate\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         if torch.isnan(loss):\n",
        "#             continue\n",
        "#         else:\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#     avg_train_loss = total_loss / len(train_dataloader) + 1\n",
        "#     train_losses.append(avg_train_loss)\n",
        "#     print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "#     # Evaluation after each epoch\n",
        "#     eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
        "#     eval_losses.append(eval_loss)"
      ],
      "metadata": {
        "id": "3hlCAVM_d6Ji"
      },
      "id": "3hlCAVM_d6Ji",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plotting the loss values\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
        "# plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Training and Evaluation Loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Ju5Un4f-d7jC"
      },
      "id": "Ju5Un4f-d7jC",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Inference\n", "\n", "(load model from pt file)"],
      "metadata": {
        "id": "sA5hEd34e7bo"
      },
      "id": "sA5hEd34e7bo"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout)  # Ensure these parameters match the original model's\n",
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt'\n",
        "model.load_state_dict(torch.load('H04Cs7O75aOfmJ4YP2HdPw.pt',map_location=torch.device('cpu')))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3yvB2Hcey5n",
        "outputId": "bee6ea19-d587-4249-f4a6-9704e04ef8a3"
      },
      "id": "O3yvB2Hcey5n",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-25 12:28:44--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13099721 (12M) [binary/octet-stream]\n",
            "Saving to: \u2018H04Cs7O75aOfmJ4YP2HdPw.pt\u2019\n",
            "\n",
            "H04Cs7O75aOfmJ4YP2H 100%[===================>]  12.49M  13.3MB/s    in 0.9s    \n",
            "\n",
            "2025-04-25 12:28:46 (13.3 MB/s) - \u2018H04Cs7O75aOfmJ4YP2HdPw.pt\u2019 saved [13099721/13099721]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (bert_embedding): BERTEmbedding(\n",
              "    (token_embedding): TokenEmbedding(\n",
              "      (embedding): Embedding(147161, 10)\n",
              "    )\n",
              "    (positional_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (segment_embedding): Embedding(3, 10)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_layer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
              "    (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
              "        (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (nextsentenceprediction): Linear(in_features=10, out_features=2, bias=True)\n",
              "  (masked_language): Linear(in_features=10, out_features=147161, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer with the BERT model's vocabulary\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
        "    # Tokenize sentences with special tokens\n",
        "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
        "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
        "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        # Assuming the model returns NSP predictions first\n",
        "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
        "        # Select the first element (first sequence) of the logits tensor\n",
        "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
        "        logits = torch.softmax(first_logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Interpret the prediction\n",
        "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"The S&P dropped 10% after the Iraqi war.\"\n",
        "sentence2 = \"I hate chocolate\"\n",
        "\n",
        "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwjBcE8IfCuR",
        "outputId": "48ff4065-1297-4931-ba0b-12e1d00dfe5c"
      },
      "id": "SwjBcE8IfCuR",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["Second sentence follows the first\n"]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mlm(sentence, model, tokenizer):\n",
        "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    tokens_tensor = inputs.input_ids\n",
        "\n",
        "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
        "    segment_labels = torch.zeros_like(tokens_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model, now correctly handling the output tuple\n",
        "        output_tuple = model(tokens_tensor, segment_labels)\n",
        "\n",
        "        # Assuming the second element of the tuple contains the MLM logits\n",
        "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
        "\n",
        "        # Identify the position of the [MASK] token\n",
        "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Get the predicted index for the [MASK] token from the MLM logits\n",
        "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
        "\n",
        "        # Replace [MASK] in the original sentence with the predicted token\n",
        "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The cat sat on the [MASK].\"\n",
        "print(predict_mlm(sentence, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epq1R2evfHEC",
        "outputId": "d7bbd0a6-6016-4810-d1e5-e75b35e6fa56"
      },
      "id": "epq1R2evfHEC",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["The cat sat on the [unused8].\n"]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["## Exercise 1: Next Sentence Prediction (NSP) with BERT"],
      "metadata": {
        "id": "8QTXrY1Zf0IN"
      },
      "id": "8QTXrY1Zf0IN"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForPreTraining\n",
        "\n",
        "# Load pretrained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pretrained model (weights)\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "# Prepare text pair for NSP\n",
        "text_1 = \"The cat sat on the mat\"\n",
        "text_2 = \"It was a sunny day\"\n",
        "# Encode text\n",
        "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
        "    nsp_logits = outputs.seq_relationship_logits\n",
        "\n",
        "# Interpret the result for NSP\n",
        "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
        "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
        "else:\n",
        "    print(\"The model thinks these sentences are consecutive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX7AaMvvfYGk",
        "outputId": "57ce18b0-68b3-49d5-88ad-979c69cba5a2"
      },
      "id": "vX7AaMvvfYGk",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["The model thinks these sentences are NOT consecutive.\n"]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["## Exercise 2: Masked Language Modeling (MLM) with BERT"],
      "metadata": {
        "id": "DQ0F5aTZgX6J"
      },
      "id": "DQ0F5aTZgX6J"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForPreTraining, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Load pretrained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pretrained model (weights)\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare text with masked token\n",
        "masked_text = \"The capital of France is [MASK].\"\n",
        "# Tokenize and prepare for the model: Convert to tokens and add special tokens\n",
        "input_ids = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids)\n",
        "    predictions = outputs.prediction_logits\n",
        "\n",
        "# Confirm we were able to predict 'Paris' as the masked token\n",
        "predicted_index = torch.argmax(predictions[0, input_ids[0] == tokenizer.mask_token_id]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
        "\n",
        "print(f\"Predicted token: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4FlvdA3gPjr",
        "outputId": "99dd40c3-028c-4f45-a3b4-029a71d9f214"
      },
      "id": "B4FlvdA3gPjr",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["Predicted token: ['paris']\n"]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
