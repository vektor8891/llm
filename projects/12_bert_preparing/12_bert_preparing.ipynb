{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/12_bert_preparing/12_bert_preparing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Text Processing for BERT\n",
        "\n",
        "## Tokenization and vocabulary building\n",
        "\n",
        "### Tokenization\n",
        "\n",
        "Components:\n",
        "\n",
        "- `tokenizer`: tokenizes text\n",
        "- `yield_tokens`: generator function that iterates through the data\n",
        "- `word_dict`: defines special tokens used in text processing, such as padding `[PAD]`, class `[CLS]`, separator `[SEP]`, mask `[MASK]`, and unknown `[UNK]`\n",
        "- `text_to_index`: converts text into a list of numerical indices based on the vocabulary\n",
        "- `index_to_en`: translates sequence of indices back into readable English text.\n",
        "\n",
        "Special tokens:\n",
        "\n",
        "- **`CLS` (Classification Token)**: Start of Sentence (SOS) marker\n",
        "- **`SEP` (Separator Token)**: End of Sentence (EOS) marker\n",
        "- **`PAD` (Padding Token)**: added to sequences to ensure all inputs are of equal length\n",
        "- **`MASK` (Masked Token)**: Utilized masked language modeling\n",
        "- **`UNK` (Unknown Token)**: placeholder for unknown words"
      ],
      "metadata": {
        "id": "E4TPkKxyDCS-"
      },
      "id": "E4TPkKxyDCS-"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.17.2\n",
        "# !pip install portalocker==2.8.2\n",
        "# !pip install transformers==4.35.2\n",
        "# !pip install torch==2.2.0"
      ],
      "metadata": {
        "id": "cEGK1BINf4JG"
      },
      "id": "cEGK1BINf4JG",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7217d6ec",
      "metadata": {
        "id": "7217d6ec"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, data_sample in data_iter:\n",
        "        yield tokenizer(data_sample)\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4\n",
        "\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["### Vocabulary building"],
      "metadata": {
        "id": "a-3UKQ77fngd"
      },
      "id": "a-3UKQ77fngd"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from itertools import chain\n",
        "from itertools import islice\n",
        "\n",
        "#create data splits\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "all_data_iter = chain(train_iter, test_iter)\n",
        "#check tokenizer\n",
        "# list(yield_tokens(all_data_iter))[5][:20]\n",
        "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
        "print(fifth_item_tokens[:20])"
      ],
      "metadata": {
        "id": "-lRLCnnafn3s",
        "outputId": "2ad1b7e5-d668-4563-f508-b69c0103906d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-lRLCnnafn3s",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'would', 'put', 'this', 'at', 'the', 'top', 'of', 'my', 'list', 'of', 'films', 'in', 'the', 'category', 'of', 'unwatchable', 'trash', '!', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#create vocab : vocab is only built using train data\n",
        "vocab=build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)\n",
        "\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "VOCAB_SIZE=len(vocab)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "QQnS4XbHfpQ9",
        "outputId": "56ec0b58-f698-46db-e6a4-ac3c63b66c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QQnS4XbHfpQ9",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["147150\n"]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])\n",
        "\n",
        "seq_en = [0, 1, 2, 3, 4, 5, 6]  # Example input sequence\n",
        "english_sentence = index_to_en(seq_en)\n",
        "seq2=[6,16,26131]\n",
        "english_sentence = index_to_en(seq2)\n",
        "\n",
        "print(english_sentence)\n",
        "\n",
        "text = \"I've seen R-rated films with male nudity. Nowhere, because they don't exist.\"  # Example input text\n",
        "text_to_index = lambda text: [vocab[token] for token in tokenizer(text)]\n",
        "index_sequence = text_to_index(text)\n",
        "\n",
        "print(index_sequence)"
      ],
      "metadata": {
        "id": "skQxzaLpsiR_",
        "outputId": "0d8a2f0d-245d-47a1-9f5e-8bae70c9cf21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "skQxzaLpsiR_",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". i speculative\n",
            "[16, 12, 149, 119, 11363, 117, 22, 928, 1047, 6, 1251, 7, 96, 42, 99, 12, 30, 1877, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text masking and data preparation for BERT\n",
        "\n",
        "### Text masking"
      ],
      "metadata": {
        "id": "KlsYBNtKszhH"
      },
      "id": "KlsYBNtKszhH"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def bernoulli_true_false(p):\n",
        "    # Create a Bernoulli distribution with probability p\n",
        "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
        "    # Sample from this distribution and convert 1 to True and 0 to False\n",
        "    return bernoulli_dist.sample().item() == 1\n",
        "\n",
        "def Masking(token):\n",
        "    # Decide whether to mask this token (20% chance)\n",
        "    mask = bernoulli_true_false(0.2)\n",
        "\n",
        "    # If mask is False, immediately return with '[PAD]' label\n",
        "    if not mask:\n",
        "        return token, '[PAD]'\n",
        "\n",
        "    # If mask is True, proceed with further operations\n",
        "    # Randomly decide on an operation (50% chance each)\n",
        "    random_opp = bernoulli_true_false(0.5)\n",
        "    random_swich = bernoulli_true_false(0.5)\n",
        "\n",
        "    # Case 1: If mask, random_opp, and random_swich are True\n",
        "    if mask and random_opp and random_swich:\n",
        "        # Replace the token with '[MASK]' and set label to a random token\n",
        "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
        "        token_ = '[MASK]'\n",
        "\n",
        "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
        "    elif mask and random_opp and not random_swich:\n",
        "        # Leave the token unchanged and set label to the same token\n",
        "        token_ = token\n",
        "        mask_label = token\n",
        "\n",
        "    # Case 3: If mask is True, but random_opp is False\n",
        "    else:\n",
        "        # Replace the token with '[MASK]' and set label to the original token\n",
        "        token_ = '[MASK]'\n",
        "        mask_label = token\n",
        "\n",
        "    return token_, mask_label"
      ],
      "metadata": {
        "id": "wKohqoaMssSo"
      },
      "id": "wKohqoaMssSo",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "for l in range(10):\n",
        "  token=\"apple\"\n",
        "  token_,label=Masking(token)\n",
        "  if token==token_ and label==\"[PAD]\":\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is left unchanged\")\n",
        "  elif token_==\"[MASK]\" and label==token:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is masked with '{token_}'\")\n",
        "  else:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is replaced with random token #{label}#\")"
      ],
      "metadata": {
        "id": "Tcemw3NNtKiy",
        "outputId": "6ddd4bef-8639-4164-ebe8-a0e6fd458880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Tcemw3NNtKiy",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK] apple \t Actual token *apple* is masked with '[MASK]'\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "[MASK] whored \t Actual token *apple* is replaced with random token #whored#\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["### Data preparation for MLM"],
      "metadata": {
        "id": "vBBfCbrrtiyJ"
      },
      "id": "vBBfCbrrtiyJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
        "    \"\"\"\n",
        "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
        "\n",
        "    \"\"\"\n",
        "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
        "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
        "    raw_tokens_list = []  # List to store raw tokens if needed\n",
        "    current_bert_input = []\n",
        "    current_bert_label = []\n",
        "    current_raw_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Apply BERT's MLM masking strategy to the token\n",
        "        masked_token, mask_label = Masking(token)\n",
        "\n",
        "        # Append the processed token and its label to the current sentence and label list\n",
        "        current_bert_input.append(masked_token)\n",
        "        current_bert_label.append(mask_label)\n",
        "\n",
        "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
        "        if include_raw_tokens:\n",
        "            current_raw_tokens.append(token)\n",
        "\n",
        "        # Check if the token is a sentence delimiter (., ?, !)\n",
        "        if token in ['.', '?', '!']:\n",
        "            # If current sentence has more than two tokens, consider it a valid sentence\n",
        "            if len(current_bert_input) > 2:\n",
        "                bert_input.append(current_bert_input)\n",
        "                bert_label.append(current_bert_label)\n",
        "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
        "                if include_raw_tokens:\n",
        "                    raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "                # Reset the lists for the next sentence\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "            else:\n",
        "                # If the current sentence is too short, discard it and reset lists\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "\n",
        "    # Add any remaining tokens as a sentence if there are any\n",
        "    if current_bert_input:\n",
        "        bert_input.append(current_bert_input)\n",
        "        bert_label.append(current_bert_label)\n",
        "        if include_raw_tokens:\n",
        "            raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "    # Return the prepared lists for BERT's MLM training\n",
        "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
      ],
      "metadata": {
        "id": "501SLFLRtONp"
      },
      "id": "501SLFLRtONp",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "original_input=\"The sun sets behind the distant mountains.\"\n",
        "tokens=tokenizer(original_input)\n",
        "bert_input, bert_label= prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "print(\"Without raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label)\n",
        "print(\"-\"*200)\n",
        "torch.manual_seed(100)\n",
        "bert_input, bert_label, raw_tokens_list= prepare_for_mlm(tokens, include_raw_tokens=True)\n",
        "print(\"With raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label,\"\\n \\t raw_tokens_list is: \\t \", raw_tokens_list)"
      ],
      "metadata": {
        "id": "KmRDSNbutoBq",
        "outputId": "a83823d1-3959-42c3-f7e5-db3d64d34cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KmRDSNbutoBq",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'whored', '[PAD]', '[PAD]']]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "With raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'whored', '[PAD]', '[PAD]']] \n",
            " \t raw_tokens_list is: \t  [['the', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["### Data preparation for NSP"],
      "metadata": {
        "id": "wL4SlfWutulw"
      },
      "id": "wL4SlfWutulw"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def process_for_nsp(input_sentences, input_masked_labels):\n",
        "    \"\"\"\n",
        "    Prepares data for Next Sentence Prediction (NSP) task in BERT training.\n",
        "\n",
        "    Args:\n",
        "    input_sentences (list): List of tokenized sentences.\n",
        "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
        "\n",
        "    Returns:\n",
        "    bert_input (list): List of sentence pairs for BERT input.\n",
        "    bert_label (list): List of masked labels for the sentence pairs.\n",
        "    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.\n",
        "    \"\"\"\n",
        "    if len(input_sentences) < 2:\n",
        "       raise ValueError(\"must have two same number of items.\")\n",
        "\n",
        "\n",
        "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
        "    if len(input_sentences) != len(input_masked_labels):\n",
        "        raise ValueError(\"Both lists must have the same number of items.\")\n",
        "\n",
        "    bert_input = []\n",
        "    bert_label = []\n",
        "    is_next = []\n",
        "\n",
        "    available_indices = list(range(len(input_sentences)))\n",
        "\n",
        "    while len(available_indices) >= 2:\n",
        "        if random.random() < 0.5:\n",
        "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
        "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
        "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
        "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
        "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(index)\n",
        "            if index + 1 in available_indices:\n",
        "                available_indices.remove(index + 1)\n",
        "        else:\n",
        "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
        "            indices = random.sample(available_indices, 2)\n",
        "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
        "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(indices[0])\n",
        "            available_indices.remove(indices[1])\n",
        "\n",
        "\n",
        "\n",
        "    return bert_input, bert_label, is_next"
      ],
      "metadata": {
        "id": "pU7zdmMKtohY"
      },
      "id": "pU7zdmMKtohY",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten the tensor\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "# Sample input sentences\n",
        "input_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\"]]\n",
        "# Create masked labels for the sentences\n",
        "input_masked_labels=[]\n",
        "for sentence in input_sentences:\n",
        "  _, current_masked_label= prepare_for_mlm(sentence, include_raw_tokens=False)\n",
        "  input_masked_labels.append(flatten(current_masked_label))\n",
        "# Create NSP pairs and labels\n",
        "random.seed(100)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n",
        "print(\"-\"*200)\n",
        "random.seed(1000)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n"
      ],
      "metadata": {
        "id": "bdfjoB0btwac",
        "outputId": "e342af52-b3c8-4e53-ebfd-7228143c758b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bdfjoB0btwac",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Input:\n",
            "[['[CLS]', 'she', 'enjoys', 'reading', 'books', '[SEP]'], ['he', 'likes', 'playing', 'guitar', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['he', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [1]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BERT Input:\n",
            "[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["### Finalizing BERT inputs"],
      "metadata": {
        "id": "ROfs5nhduDhP"
      },
      "id": "ROfs5nhduDhP"
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
        "    \"\"\"\n",
        "    Prepare the final input lists for BERT training.\n",
        "    \"\"\"\n",
        "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
        "        pair=deepcopy(pair_)\n",
        "        max_len = max(len(pair[0]), len(pair[1]))\n",
        "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
        "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
        "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
        "        return pair[0], pair[1]\n",
        "\n",
        "    #flatten the tensor\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    #transform tokens to vocab indices\n",
        "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
        "\n",
        "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
        "\n",
        "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
        "        # Create segment labels for each pair of sentences\n",
        "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
        "\n",
        "        # Zero-pad the bert_input and bert_label and segment_label\n",
        "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
        "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
        "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
        "\n",
        "        #convert to tensors\n",
        "        if to_tenor:\n",
        "\n",
        "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
        "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
        "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "        else:\n",
        "          # Flatten the padded inputs and labels\n",
        "            bert_inputs_final.append(flatten(bert_input_padded))\n",
        "            bert_labels_final.append(flatten(bert_label_padded))\n",
        "            segment_labels_final.append(flatten(segment_label_padded))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final"
      ],
      "metadata": {
        "id": "YZAgCX96uAeC"
      },
      "id": "YZAgCX96uAeC",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final=prepare_bert_final_inputs(bert_input, bert_label, is_next,to_tenor=True)\n",
        "torch.set_printoptions(linewidth=10000)# this assures that whole output is printed in one line\n",
        "print(\"input:\\t\\t\",bert_input,\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nbert labels final:\\t\",bert_labels_final,\"\\nsegment labels final:\\t\",segment_labels_final,\"\\nis nexts final:\\t\",is_nexts_final)"
      ],
      "metadata": {
        "id": "2Ki2RSCTuIkb",
        "outputId": "b34aaaff-28ea-445e-c707-6ebdac9ee9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2Ki2RSCTuIkb",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "inputs_final:\t [tensor([    1,    33,  1155,   404,  4833,     2,    16,   123, 14227,     2,     0,     0])] \n",
            "bert labels final:\t [tensor([ 0, 33,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])] \n",
            "segment labels final:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])] \n",
            "is nexts final:\t [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input:\\t\\t\",bert_input,\"\\nmask_label:\\t\",bert_label, \"\\nlabels_final: \\t\",bert_labels_final)"
      ],
      "metadata": {
        "id": "6FpdZRh-uPNE",
        "outputId": "9ce39b7d-8c4c-4241-d518-ba8a7ac0214b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6FpdZRh-uPNE",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "mask_label:\t [[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]] \n",
            "labels_final: \t [tensor([ 0, 33,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nsegment_labels:\\t\",segment_labels_final)"
      ],
      "metadata": {
        "id": "SjMh2p01uRzj",
        "outputId": "7ea427db-80bb-4531-c65d-4bec72e53ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SjMh2p01uRzj",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "inputs_final:\t [tensor([    1,    33,  1155,   404,  4833,     2,    16,   123, 14227,     2,     0,     0])] \n",
            "segment_labels:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["## Preparing the dataset"],
      "metadata": {
        "id": "riXZv1CouZoY"
      },
      "id": "riXZv1CouZoY"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "csv_file_path ='train_bert_data_new.csv'\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
        "\n",
        "    # Wrap train_iter with tqdm for a progress bar\n",
        "    for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
        "        # Tokenize the sample input\n",
        "        tokens = tokenizer(sample)\n",
        "        # Create MLM inputs and labels\n",
        "        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "        if len(bert_input) < 2:\n",
        "            continue\n",
        "        # Create NSP pairs, token labels, and is_next label\n",
        "        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
        "        # add zero-paddings, map tokens to vocab indices and create segment labels\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
        "        # convert tensors to lists, convert lists to JSON-formatted strings\n",
        "        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
        "            bert_input_str = json.dumps(bert_input.tolist())\n",
        "            bert_label_str = json.dumps(bert_label.tolist())\n",
        "            segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
        "            # Write the data to a CSV file row-by-row\n",
        "            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])"
      ],
      "metadata": {
        "id": "JHUxMAFLuTN_",
        "outputId": "e2b4618a-2a6c-4119-a497-dfc8501cb697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "id": "JHUxMAFLuTN_",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": ["Processing samples: 900it [06:57,  2.16it/s]\n"]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-953c091c79f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Create MLM inputs and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mbert_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_for_mlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_raw_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e82985130813>\u001b[0m in \u001b[0;36mprepare_for_mlm\u001b[0;34m(tokens, include_raw_tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Apply BERT's MLM masking strategy to the token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmasked_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Append the processed token and its label to the current sentence and label list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0871663b2dee>\u001b[0m in \u001b[0;36mMasking\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrandom_opp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrandom_swich\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Replace the token with '[MASK]' and set label to a random token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmask_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtoken_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a71a84b35cb3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(seq_en)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_to_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex_to_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mseq_en\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_en\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseq_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Example input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menglish_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a71a84b35cb3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_to_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex_to_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mseq_en\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_en\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseq_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Example input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menglish_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/vocab/vocab.py\u001b[0m in \u001b[0;36mget_itos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__prepare_scriptable__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1 - Initializing the BERTTokenizer"
      ],
      "metadata": {
        "id": "x9OYu_PYve4F"
      },
      "id": "x9OYu_PYve4F"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "8nmoWYHHuecP",
        "outputId": "916fc805-2ac1-4f16-8881-de41795e62c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "6506da212de748a69a04630f79caabed",
            "b286eb691a464dc5b89ca83807847414",
            "36f4ea51fb83491c8a763175451226f7",
            "69e8415fcfbd43bd9261c34d4b9c10cd",
            "33a2265a8d0144188ea9f39b11e7943d",
            "43cd80a724d34b5ea5d0677e6b579403",
            "66581931925e4aa496a0103a07b9c9dc",
            "0cd41e83a8104d00ac6e90dbdd84e000",
            "4432c46bb38c42218bb115a45292b3f6",
            "0bdd8dbb276a4ba1b6a183505091a3a7",
            "82eb1856d6004391974881765cef9a9f",
            "8e86f28e3cd44818b0e2ca6b3a7edbf1",
            "127a13fa33a0477393a073d144396360",
            "160ed15a4716410a8de5d93715a1c2ff",
            "bd51efb29820471da7d911ba697558ea",
            "3fa12909dd0140939b6dd943b296a668",
            "47bbfac11fe1421888b0aebb17045a24",
            "aa5d81eb7aba4a569ccc7d8c48f777b0",
            "53ca9304fceb41ec93b860049eb1d820",
            "0c039314d01a4734b90105b2ef54f7dd",
            "0d70696c33f24bdebd85106d0efb6f74",
            "bfa34ea5d3554373a3ebe2e598310b63",
            "900ccabeae234c84b652e9644dfd411a",
            "b3733dfc4d6b44b19f80f906b3192173",
            "d2acc4ac24fa4c2e8d710acc96caf76f",
            "45c9099aa35f45b2bada6e87a56ec051",
            "60c4cf7d51284c0da8a43a9cbb955acf",
            "d7c5efd03ee047048bb6b66c50b5c6d9",
            "cac2e15f8fd54481bbb54d42657b913a",
            "db8a216152784668a2144a65cd745e41",
            "13f5f1b17db64b56a8694460d9f7b6e1",
            "bdb8c343eea44afa846bfdf7d9b7bb04",
            "b3c2fc3dd1ff45dfb154fc8b088fccca",
            "fc68f33fdde442e8bfab9a861d0ea1a3",
            "c267318a4cb94e0bb85bdc8be5bb7884",
            "6c6e31a42f3045e59d8ea523b99b823b",
            "8e7ab17829594dcbb607bb142e761adc",
            "6a737823832a439d84c0a1f8e62c000b",
            "af0263860b4947e0af61864b9a6f55f7",
            "e6a4f347783c4493b098a680b74d9451",
            "8d1bbbf424184fac99c33a83bf8f1d27",
            "58b7202166fe48c0997424efad4b7340",
            "185a9436e5144467adbbba1325db817e",
            "5205720280554437926d957dce423011"
          ]
        }
      },
      "id": "8nmoWYHHuecP",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6506da212de748a69a04630f79caabed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e86f28e3cd44818b0e2ca6b3a7edbf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "900ccabeae234c84b652e9644dfd411a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc68f33fdde442e8bfab9a861d0ea1a3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": ["### Exercise 2 - Tokenizing the dataset"],
      "metadata": {
        "id": "WpSVaROTvwiH"
      },
      "id": "WpSVaROTvwiH"
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for _, data_sample in data_iter:\n",
        "        # Use the BERT tokenizer to tokenize the text\n",
        "        # This returns a dictionary with 'input_ids' among other things\n",
        "        tokens = tokenizer(data_sample, return_tensors='pt', truncation=True, max_length=512)['input_ids'][0]\n",
        "        yield tokens.tolist()"
      ],
      "metadata": {
        "id": "zR1ljcR7vqwK"
      },
      "id": "zR1ljcR7vqwK",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 - Building the vocabulary with special tokens"
      ],
      "metadata": {
        "id": "ZVDBD3eFv4_x"
      },
      "id": "ZVDBD3eFv4_x"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX, CLS_IDX, SEP_IDX, MASK_IDX, UNK_IDX = tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.mask_token_id, tokenizer.unk_token_id\n",
        "special_symbols = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        "\n",
        "# Load IMDB dataset\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "\n",
        "# Convert to map-style datasets to be compatible with transformers' tokenizers\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Since you are using a pre-trained tokenizer, you don't need to build the vocab from scratch.\n",
        "# Instead, you can directly use the tokenizer's vocab.\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "print(\"Vocabulary Size:\", VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "3vzaLyPGv2rC",
        "outputId": "e25bd9c8-1127-48f2-a2e0-d82aa6ffa09f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3vzaLyPGv2rC",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ["Vocabulary Size: 30522\n"]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": ["!pip freeze > requirements.txt"],
      "metadata": {
        "id": "JP_55tH8wUrI",
        "outputId": "0df790b0-11f9-41b1-e8cd-cb6a9091ddc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JP_55tH8wUrI",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
