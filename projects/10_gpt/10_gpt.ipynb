{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/10_gpt/10_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.15.1\n",
        "# !pip install portalocker\n",
        "# !pip install transformers\n",
        "# !pip install --upgrade torch"
      ],
      "metadata": {
        "id": "nUd6MmhF_-Ri"
      },
      "id": "nUd6MmhF_-Ri",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text pipeline\n",
        "## Dataset"
      ],
      "metadata": {
        "id": "jp9g4GFg_zaF"
      },
      "id": "jp9g4GFg_zaF"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a0947318",
      "metadata": {
        "id": "a0947318",
        "outputId": "c4b41b16-f514-42a3-c52a-3fa722c9f9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-762cb4c6fd95>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_torch_home\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "\n",
        "# Load the dataset\n",
        "train_iter, val_iter = IMDB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae24860f",
      "metadata": {
        "id": "ae24860f"
      },
      "outputs": [],
      "source": [
        "data_itr=iter(train_iter)\n",
        "# retrieving the third first record\n",
        "next(data_itr)\n",
        "next(data_itr)\n",
        "next(data_itr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "Hf7mxFi9A7KR"
      },
      "id": "Hf7mxFi9A7KR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data"
      ],
      "metadata": {
        "id": "mkFJfJW4BsvB"
      },
      "id": "mkFJfJW4BsvB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
      ],
      "metadata": {
        "id": "bYQMKgJ7BsU3"
      },
      "id": "bYQMKgJ7BsU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "5trXDm52BkpC"
      },
      "id": "5trXDm52BkpC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "\n",
        "    for _,data_sample in data_iter:\n",
        "        yield  tokenizer(data_sample)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
        "vocab.set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "_B4vrLbwBz2Y"
      },
      "id": "_B4vrLbwBz2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Text to index and index to Text"
      ],
      "metadata": {
        "id": "e1e-_-qRC1Sh"
      },
      "id": "e1e-_-qRC1Sh"
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "metadata": {
        "id": "8rkuKSjwB5Rn"
      },
      "id": "8rkuKSjwB5Rn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "index_to_en(torch.tensor([0,1,2]))"
      ],
      "metadata": {
        "id": "VF-tFTKuC3uW"
      },
      "id": "VF-tFTKuC3uW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collate function"
      ],
      "metadata": {
        "id": "rKtMsk25DCZy"
      },
      "id": "rKtMsk25DCZy"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample(block_size, text):\n",
        "    # Determine the length of the input text\n",
        "    sample_leg = len(text)\n",
        "    # Calculate the stopping point for randomly selecting a sample\n",
        "    # This ensures the selected sample doesn't exceed the text length\n",
        "    random_sample_stop = sample_leg - block_size\n",
        "\n",
        "\n",
        "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
        "    if random_sample_stop >= 1:\n",
        "        # Randomly select a starting point for the sample\n",
        "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
        "        # Define the endpoint of the sample\n",
        "        stop = random_start + block_size\n",
        "\n",
        "        # Create the input and target sequences\n",
        "        src_sequence = text[random_start:stop]\n",
        "        tgt_sequence= text[random_start + 1:stop + 1]\n",
        "\n",
        "    # Handle the case where the text length is exactly equal or less the block size\n",
        "    elif random_sample_stop <= 0:\n",
        "        # Start from the beginning and use the entire text\n",
        "        random_start = 0\n",
        "        stop = sample_leg\n",
        "        src_sequence= text[random_start:stop]\n",
        "        tgt_sequence = text[random_start + 1:stop]\n",
        "        # Append an empty string to maintain sequence alignment\n",
        "        tgt_sequence.append( '<|endoftext|>')\n",
        "\n",
        "    return src_sequence, tgt_sequence"
      ],
      "metadata": {
        "id": "gLOi7hJ6C7X0"
      },
      "id": "gLOi7hJ6C7X0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=1\n",
        "\n",
        "batch_of_tokens=[]\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  _,text =next(iter(train_iter))\n",
        "  batch_of_tokens.append(tokenizer(text))"
      ],
      "metadata": {
        "id": "buYKCfpRDETB"
      },
      "id": "buYKCfpRDETB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=batch_of_tokens[0][0:100]\n",
        "text[0:100]\n",
        "batch_of_tokens"
      ],
      "metadata": {
        "id": "saEpbN5oDF5z"
      },
      "id": "saEpbN5oDF5z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=10\n",
        "src_sequences, tgt_sequence=get_sample( block_size, text)"
      ],
      "metadata": {
        "id": "nRYzNzCQDKax"
      },
      "id": "nRYzNzCQDKax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"src: \",src_sequences)\n",
        "print(\"tgt: \",tgt_sequence)"
      ],
      "metadata": {
        "id": "7DyyiDM4DZJa"
      },
      "id": "7DyyiDM4DZJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store source and target sequences\n",
        "src_batch, tgt_batch = [], []\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Loop to create batches of source and target sequences\n",
        "for i in range(BATCH_SIZE):\n",
        "    # Retrieve the next data point from the training iterator\n",
        "    _,text = next(iter(train_iter))\n",
        "\n",
        "    # Generate source and target sequences using the get_sample function\n",
        "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
        "\n",
        "    # Convert source and target sequences to tokenized vocabulary indices\n",
        "    src_sequence_indices = vocab(src_sequence_text)\n",
        "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
        "\n",
        "    # Convert the sequences to PyTorch tensors with dtype int64\n",
        "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
        "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
        "\n",
        "    # Append the source and target sequences to their respective batches\n",
        "    src_batch.append(src_sequence)\n",
        "    tgt_batch.append(tgt_sequence)\n",
        "\n",
        "    # Print the output for every 2nd sample (adjust as needed)\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
        "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
        "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
        "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
        "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
        "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
      ],
      "metadata": {
        "id": "0oINQOoZDamD"
      },
      "id": "0oINQOoZDamD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "BLOCK_SIZE=30\n",
        "def collate_batch(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for _,_textt in batch:\n",
        "      src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
        "      src_sequence=vocab(src_sequence)\n",
        "      tgt_sequence=vocab(tgt_sequence)\n",
        "      src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
        "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
        "      src_batch.append(src_sequence)\n",
        "      tgt_batch.append(tgt_sequence)\n",
        "\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "\n",
        "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
      ],
      "metadata": {
        "id": "7VtAjlpxDj3m"
      },
      "id": "7VtAjlpxDj3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE=1\n",
        "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader= DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "t1I4yiq5EBOD"
      },
      "id": "t1I4yiq5EBOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterating through data samples"
      ],
      "metadata": {
        "id": "DDSUa9WrEPpW"
      },
      "id": "DDSUa9WrEPpW"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=iter(dataloader)\n",
        "for sample in range(10):\n",
        "  src,trt=next(dataset)\n",
        "  print(\"sample\",sample)\n",
        "  print(\"sorce:\",index_to_en(src))\n",
        "  print(\"\\n\")\n",
        "  print(\"target:\",index_to_en(trt))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "qViP1Y62EHrS"
      },
      "id": "qViP1Y62EHrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for  src,trt in dataset:\n",
        "    print(trt.shape)\n",
        "    print(src.shape)\n",
        "    print(index_to_en(src[0,:]))\n",
        "    print(index_to_en(trt[0,:]))\n",
        "    break"
      ],
      "metadata": {
        "id": "PeWG2FQG1oDt"
      },
      "id": "PeWG2FQG1oDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"source:\",index_to_en(src))\n",
        "print(\"target:\",index_to_en(trt))"
      ],
      "metadata": {
        "id": "U5u243qe1pp5"
      },
      "id": "U5u243qe1pp5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking"
      ],
      "metadata": {
        "id": "CLfhsHCV1uTJ"
      },
      "id": "CLfhsHCV1uTJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "8bfvyvRL1utw"
      },
      "id": "8bfvyvRL1utw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(src,device=DEVICE):\n",
        "    src_seq_len = src.shape[0]\n",
        "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask,src_padding_mask"
      ],
      "metadata": {
        "id": "uhuJyLtn1xiM"
      },
      "id": "uhuJyLtn1xiM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encoding"
      ],
      "metadata": {
        "id": "wRsbcfW5104B"
      },
      "id": "wRsbcfW5104B"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "# add positional information to the input tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "Lr8NmNWD11R8"
      },
      "id": "Lr8NmNWD11R8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token embedding"
      ],
      "metadata": {
        "id": "LQwJl5KS15E8"
      },
      "id": "LQwJl5KS15E8"
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "JeHgK_Tw15ZW"
      },
      "id": "JeHgK_Tw15ZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom GPT model architecture\n",
        "\n",
        "`CustomGPTModel`: transformer-based model architecture for generative pre-trained models. Purpose: to generate text and perform various NLP tasks. Main components:\n",
        "\n",
        "- **Initialization (`__init__`)**: initializes the embedding layer, positional encoding, transformer encoder layers, and a linear layer (`lm_head`) for generating logits over the vocabulary.\n",
        "\n",
        "- **Weight initialization (`init_weights`)**: initializes model weights using Xavier uniform initialization.\n",
        "\n",
        "- **Decoder (`decoder`)**: currently functions as the forward pass through the transformer encoder layers, followed by the generation of logits for the language modeling task. Adds positional encodings to the embeddings and applies a mask if necessary.\n",
        "\n",
        "- **Forward pass (`forward`)**: similar to `decoder`. Defines the forward computation of the model. Processes the input through embedding layers, positional encoding, transformer encoder layers, and produces the final output using the `lm_head`.\n",
        "\n",
        "- **Mask generation**: included in both `decoder` and `forward`. Purpose: to ensure prediction does not depend on future tokens."
      ],
      "metadata": {
        "id": "okX2mTD517Vi"
      },
      "id": "okX2mTD517Vi"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class CustomGPTModel(nn.Module):\n",
        "    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.init_weights()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
        "\n",
        "        # Remaining layers are part of the TransformerDecoder\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.embed_size = embed_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def init_weights(self):\n",
        "      for p in self.parameters():\n",
        "          if p.dim() > 1:\n",
        "              nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_mask(src,device=DEVICE):\n",
        "        src_seq_len = src.shape[0]\n",
        "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
        "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "        return src_mask,src_padding_mask\n",
        "\n",
        "    def decoder(self, x,src_mask):\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def forward(self,x,src_mask=None,key_padding_mask=None):\n",
        "\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask,key_padding_mask)\n",
        "        x = self.lm_head(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OqC2dnrD19qo"
      },
      "id": "OqC2dnrD19qo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model configuration and initialization\n",
        "\n",
        "- `ntokens`: # unique tokens in the vocabulary\n",
        "- `emsize`: size of each embedding vector\n",
        "- `nlayers`: # transformer encoder layers\n",
        "- `nhead`: # attention heads\n",
        "- `dropout`: regularization technique to ignore randomly selected neurons during training to prevent overfitting\n"
      ],
      "metadata": {
        "id": "5Ypk4_Gw5ywt"
      },
      "id": "5Ypk4_Gw5ywt"
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)"
      ],
      "metadata": {
        "id": "cSi1ssBZ5vnD"
      },
      "id": "cSi1ssBZ5vnD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting\n",
        "\n",
        "Prompt: starting point for the model to generate text."
      ],
      "metadata": {
        "id": "5czGvAzn6ZYR"
      },
      "id": "5czGvAzn6ZYR"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
        "    # Handle None prompt\n",
        "    while prompt is None:\n",
        "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
        "\n",
        "    tokens = tokenizer(prompt)\n",
        "    number_of_tokens = len(tokens)\n",
        "\n",
        "    # Handle long prompts\n",
        "    if number_of_tokens > block_size:\n",
        "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
        "\n",
        "    prompt_indices = vocab(tokens)\n",
        "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
        "    return prompt_encoded"
      ],
      "metadata": {
        "id": "_Fr3Dwag6HTU"
      },
      "id": "_Fr3Dwag6HTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_to_en(encode_prompt(None)))"
      ],
      "metadata": {
        "id": "IijibOHh6kdv"
      },
      "id": "IijibOHh6kdv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_to_en(encode_prompt(\"This is a prompt to get model generate next words.\" ) ))"
      ],
      "metadata": {
        "id": "OEbNOcw16l0-"
      },
      "id": "OEbNOcw16l0-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_encoded=encode_prompt(\"This is a prompt to get model generate next words.\").to(DEVICE)\n",
        "prompt_encoded"
      ],
      "metadata": {
        "id": "UNdQkjGM6oab"
      },
      "id": "UNdQkjGM6oab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.decoder(prompt_encoded,src_mask=None).to(DEVICE)\n",
        "logits = logits.transpose(0, 1)\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "y9vxOymP6rEY"
      },
      "id": "y9vxOymP6rEY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_preiction =logits[:,-1]\n",
        "logit_preiction.shape"
      ],
      "metadata": {
        "id": "a7KuXKgT6u5Y"
      },
      "id": "a7KuXKgT6u5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " _, next_word_index = torch.max(logit_preiction, dim=1)\n",
        " next_word_index"
      ],
      "metadata": {
        "id": "8siF8Co361ts"
      },
      "id": "8siF8Co361ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_en(next_word_index)"
      ],
      "metadata": {
        "id": "KG_avLoZ66Ec"
      },
      "id": "KG_avLoZ66Ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregressive text generation"
      ],
      "metadata": {
        "id": "2rUbCFDI7IW0"
      },
      "id": "2rUbCFDI7IW0"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"this is the beginning of\"\n",
        "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "print(\"Device for prompt_encoded:\", prompt_encoded.shape)"
      ],
      "metadata": {
        "id": "r14ecnUk7IHL"
      },
      "id": "r14ecnUk7IHL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens=10\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    logits = model.decoder(prompt_encoded,src_mask=None)\n",
        "    logits = logits.transpose(0, 1)\n",
        "    print(\" \")\n",
        "    print(f\"Shape of logits at step {i}: {logits.shape}\")\n",
        "\n",
        "    logit_preiction = logits[:, -1]\n",
        "    print(f\"Shape of logit_prediction at step {i}: {logit_preiction.shape}\")\n",
        "\n",
        "    next_token_encoded = torch.argmax(logit_preiction, dim=-1).reshape(-1, 1)\n",
        "    print(f\"Shape of next_token_encoded at step {i}: {next_token_encoded.shape}\")\n",
        "\n",
        "    prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0).to(DEVICE)\n",
        "    print(f\"Sequence for step {i}: {[index_to_en(j) for j in prompt_encoded]}\")\n",
        "    print(f\"Shape of prompt_encoded after concatenation at step {i}: {prompt_encoded.shape}\")"
      ],
      "metadata": {
        "id": "ANjr8K4O67eK"
      },
      "id": "ANjr8K4O67eK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]\n",
        "BLOCK_SIZE"
      ],
      "metadata": {
        "id": "cCtuaM4T6_u_"
      },
      "id": "cCtuaM4T6_u_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#auto-regressive Language Model text generation\n",
        "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
        "    # Move model to the specified device (e.g., GPU or CPU)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Encode the input prompt using the provided encode_prompt function\n",
        "    prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "    tokens = []\n",
        "\n",
        "    # Generate new tokens up to max_new_tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Decode the encoded prompt using the model's decoder\n",
        "        logits = model(prompt_encoded,src_mask=None,key_padding_mask=None)\n",
        "\n",
        "        # Transpose the logits to bring the sequence length to the first dimension\n",
        "        logits = logits.transpose(0, 1)\n",
        "\n",
        "        # Select the logits of the last token in the sequence\n",
        "        logit_prediction = logits[:, -1]\n",
        "\n",
        "        # Choose the most probable next token from the logits(greedy decoding)\n",
        "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
        "\n",
        "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
        "        if next_token_encoded.item() == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
        "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
        "\n",
        "        # Convert the next token index to a token string using the vocabulary\n",
        "        # Move the tensor back to CPU for vocab lookup if needed\n",
        "        token_id = next_token_encoded.to('cpu').item()\n",
        "        tokens.append(vocab.get_itos()[token_id])\n",
        "\n",
        "    # Join the generated tokens into a single string and return\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "iP22l4_x7nf3"
      },
      "id": "iP22l4_x7nf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "u52fuxyj7oZ2"
      },
      "id": "u52fuxyj7oZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding the differences: Training vs. inference\n",
        "\n",
        "- Training: using ground truth (\"teacher forcing\")\n",
        "- Interence: use previous predictions"
      ],
      "metadata": {
        "id": "TOSZ98lW8P1W"
      },
      "id": "TOSZ98lW8P1W"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "f7tF-JEx7qdB"
      },
      "id": "f7tF-JEx7qdB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src,tgt=next(iter(dataloader))\n",
        "\n",
        "mask,padding_mask = create_mask(src)"
      ],
      "metadata": {
        "id": "Bx5712Dq89zt"
      },
      "id": "Bx5712Dq89zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(src,src_mask=mask,key_padding_mask=padding_mask)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "w_uTUXCW8_pN"
      },
      "id": "w_uTUXCW8_pN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"output shape\",logits.shape)\n",
        "print(\"source shape \",src)"
      ],
      "metadata": {
        "id": "74-JfxnT9COW"
      },
      "id": "74-JfxnT9COW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the the first sample of the target\n",
        "tgt\n",
        "print(tgt.shape)\n",
        "print(logits.reshape(-1, logits.shape[-1]).shape)\n",
        "print(tgt.reshape(-1).shape)"
      ],
      "metadata": {
        "id": "M1XrYveR9D3I"
      },
      "id": "M1XrYveR9D3I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in eval_data:\n",
        "            tgt = tgt.to(DEVICE)\n",
        "            #seq_len = src.size(0)\n",
        "            logits = model(src,src_mask=None,key_padding_mask=None)\n",
        "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
        "    return total_loss / (len(list(eval_data)) - 1)"
      ],
      "metadata": {
        "id": "OSVIBgBf9Tk9"
      },
      "id": "OSVIBgBf9Tk9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model,val_dataloader)"
      ],
      "metadata": {
        "id": "VZmTTlQg9VFU"
      },
      "id": "VZmTTlQg9VFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "(Only if you have GPU)"
      ],
      "metadata": {
        "id": "7yRF144V9bNt"
      },
      "id": "7yRF144V9bNt"
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import Adam\n",
        "\n",
        "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
        "\n",
        "# def train(model: nn.Module,train_data) -> None:\n",
        "#     model.train()  # turn on train mode\n",
        "#     total_loss = 0.\n",
        "#     log_interval = 10000\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     num_batches = len(list(train_data)) // block_size\n",
        "#     for batch,srctgt in enumerate(train_data):\n",
        "#         src= srctgt[0]\n",
        "#         tgt= srctgt[1]\n",
        "#         logits = model(src,src_mask=None)\n",
        "#         logits_flat = logits.reshape(-1, logits.shape[-1])\n",
        "#         loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
        "#             lr = scheduler.get_last_lr()[0]\n",
        "#             ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "#             #cur_loss = total_loss / log_interval\n",
        "#             cur_loss = total_loss / batch\n",
        "#             ppl = math.exp(cur_loss)\n",
        "#             print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
        "#                   f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "#                   f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "#             start_time = time.time()\n",
        "\n",
        "#     return total_loss"
      ],
      "metadata": {
        "id": "b9lDyoL5F7mZ"
      },
      "id": "b9lDyoL5F7mZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "\n",
        "# best_val_loss = float('inf')\n",
        "# epochs = 30\n",
        "# Train_losses= []\n",
        "# Val_losses = []\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     epoch_start_time = time.time()\n",
        "#     train_loss = train(model,dataloader)\n",
        "#     val_loss = evaluate(model, val_dataloader)\n",
        "#     val_ppl = math.exp(val_loss)\n",
        "#     Train_losses.append(train_loss)\n",
        "#     Val_losses.append(val_loss)\n",
        "\n",
        "#     elapsed = time.time() - epoch_start_time\n",
        "#     print('-' * 89)\n",
        "#     print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "#         f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "#     print('-' * 89)\n",
        "\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         torch.save(model.state_dict(), 'model_best_val_loss.pt')"
      ],
      "metadata": {
        "id": "qMNUHcno9V91"
      },
      "id": "qMNUHcno9V91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
        "# num_epochs = len(Train_losses)\n",
        "\n",
        "# # Create a figure and a set of subplots\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# # Plot the training losses\n",
        "# ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
        "\n",
        "# # Plot the validation losses\n",
        "# ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
        "\n",
        "# # Set the x-axis label\n",
        "# ax.set_xlabel('Epoch')\n",
        "\n",
        "# # Set the y-axis label\n",
        "# ax.set_ylabel('Loss')\n",
        "\n",
        "# # Set the title of the plot\n",
        "# ax.set_title('Training and Validation Losses')\n",
        "\n",
        "# # Add a legend to the plot\n",
        "# ax.legend()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "RwqEA0d69eo2"
      },
      "id": "RwqEA0d69eo2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the saved model"
      ],
      "metadata": {
        "id": "pVX5BUuQIxeI"
      },
      "id": "pVX5BUuQIxeI"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt'\n",
        "model.load_state_dict(torch.load('kyn1_OsXrzjef0xihlsXmg.pt',map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "ze4Lc8e8IxGF"
      },
      "id": "ze4Lc8e8IxGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model,prompt=\"the movie was\",max_new_tokens=10,vocab=vocab,tokenizer=tokenizer))"
      ],
      "metadata": {
        "id": "6eyDwRI4GBiK"
      },
      "id": "6eyDwRI4GBiK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading GPT2 model from HuggingFace"
      ],
      "metadata": {
        "id": "_6lgIqClI8uz"
      },
      "id": "_6lgIqClI8uz"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define the input prompt\n",
        "#input_text = \"Once upon a time in a faraway land,\"\n",
        "input_text = \"the movie was\"\n",
        "\n",
        "# Tokenize the input text and prepare the input for the model\n",
        "input_ids = tokenizer1.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text using the model\n",
        "# Set the desired length of the generated text (max_length),\n",
        "# and other generation parameters like temperature, top_k, and top_p\n",
        "max_length = 15\n",
        "temperature = 0.7\n",
        "top_k = 50\n",
        "top_p = 0.95\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        "    pad_token_id=tokenizer1.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer1.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the input prompt and the generated text\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Generated Text: {generated_text}\")"
      ],
      "metadata": {
        "id": "H-95sVyKI4dj"
      },
      "id": "H-95sVyKI4dj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0jOFFQ7I-Ty"
      },
      "id": "o0jOFFQ7I-Ty",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}