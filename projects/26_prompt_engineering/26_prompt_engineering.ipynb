{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/26_prompt_engineering/26_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -qq ibm-watsonx-ai\n",
        "# ! pip install -qq langchain\n",
        "# ! pip install -qq langchain-ibm\n",
        "# ! pip install -qq git+https://github.com/ibm-granite-community/utils"
      ],
      "metadata": {
        "id": "J8uafqA4q7vE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb82cfd4-a810-4e94-c79c-92d747e83b3d"
      },
      "id": "J8uafqA4q7vE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-Context Engineering and Prompt Templates"
      ],
      "metadata": {
        "id": "xzm0KzZ5qcZh"
      },
      "id": "xzm0KzZ5qcZh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup LLM"
      ],
      "metadata": {
        "id": "HtVLFqpcqycM"
      },
      "id": "HtVLFqpcqycM"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7184dd7",
      "metadata": {
        "id": "e7184dd7"
      },
      "outputs": [],
      "source": [
        "from langchain_ibm.llms import WatsonxLLM\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from google.colab import userdata\n",
        "\n",
        "def llm_model(prompt_txt, params=None):\n",
        "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "\n",
        "    default_params = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"min_new_tokens\": 0,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.2,\n",
        "        \"top_k\": 1\n",
        "    }\n",
        "\n",
        "    if params:\n",
        "        default_params.update(params)\n",
        "\n",
        "    parameters = {\n",
        "        GenParams.MAX_NEW_TOKENS: default_params[\"max_new_tokens\"],  # this controls the maximum number of tokens in the generated output\n",
        "        GenParams.MIN_NEW_TOKENS: default_params[\"min_new_tokens\"], # this controls the minimum number of tokens in the generated output\n",
        "        GenParams.TEMPERATURE: default_params[\"temperature\"], # this randomness or creativity of the model's responses\n",
        "        GenParams.TOP_P: default_params[\"top_p\"],\n",
        "        GenParams.TOP_K: default_params[\"top_k\"]\n",
        "    }\n",
        "\n",
        "    api_key = userdata.get('IBM_CLOUD_API_KEY')\n",
        "\n",
        "    credentials = {\n",
        "      \"url\": userdata.get(\"WATSONX_URL\"),\n",
        "      \"apikey\": userdata.get('IBM_CLOUD_API_KEY')\n",
        "  }\n",
        "\n",
        "    project_id = userdata.get(\"WATSONX_PROJECT_ID\")\n",
        "\n",
        "    mixtral_llm = WatsonxLLM(\n",
        "        model_id=model_id,\n",
        "        url=userdata.get(\"WATSONX_URL\"),\n",
        "        apikey=userdata.get('IBM_CLOUD_API_KEY'),\n",
        "        project_id=userdata.get(\"WATSONX_PROJECT_ID\"),\n",
        "        params=parameters\n",
        "    )\n",
        "\n",
        "    response  = mixtral_llm.invoke(prompt_txt)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GenParams().get_example_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxi-SrBCq1ri",
        "outputId": "11bf8c87-141b-4574-ac7d-24e8a71b5832"
      },
      "id": "mxi-SrBCq1ri",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'decoding_method': 'sample',\n",
              " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
              " 'temperature': 0.5,\n",
              " 'top_p': 0.2,\n",
              " 'top_k': 1,\n",
              " 'random_seed': 33,\n",
              " 'repetition_penalty': 2,\n",
              " 'min_new_tokens': 50,\n",
              " 'max_new_tokens': 200,\n",
              " 'stop_sequences': ['fail'],\n",
              " 'time_limit': 600000,\n",
              " 'truncate_input_tokens': 200,\n",
              " 'prompt_variables': {'object': 'brain'},\n",
              " 'return_options': {'input_text': True,\n",
              "  'generated_tokens': True,\n",
              "  'input_tokens': True,\n",
              "  'token_logprobs': True,\n",
              "  'token_ranks': False,\n",
              "  'top_n_tokens': False}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt engineering"
      ],
      "metadata": {
        "id": "gywz1YvuroJ9"
      },
      "id": "gywz1YvuroJ9"
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 128,\n",
        "    \"min_new_tokens\": 10,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\": 0.2,\n",
        "    \"top_k\": 1\n",
        "}\n",
        "\n",
        "prompt = \"The wind is\"\n",
        "\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwsrDANgrmEs",
        "outputId": "6b4fb0bd-bbeb-4e7e-a4cc-cd62ec281f7d"
      },
      "id": "qwsrDANgrmEs",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:436: LifecycleWarning: Model 'mistralai/mixtral-8x7b-instruct-v01' is in deprecated state from 2025-04-30 until 2025-07-30. IDs of alternative models: mistralai/mistral-small-3-1-24b-instruct-2503. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warn(model_state_warning, category=LifecycleWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: The wind is\n",
            "\n",
            "response :  howling outside, and the rain is coming down in sheets. It’s the perfect weather for a cozy night in with a good book. But what to read? If you’re looking for something to take your mind off the storm, here are five great books to curl up with on a rainy night.\n",
            "\n",
            "## 1. The Secret History by Donna Tartt\n",
            "\n",
            "This gripping novel tells the story of a group of classics students at an elite New England college who become embroiled in a murder conspiracy. With its richly drawn characters and atmospheric setting, The Secret History is\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}