{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/06_word2vec/06_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.15.1"
      ],
      "metadata": {
        "id": "bOITVD7n7g8z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchtext.vocab import GloVe, vocab, build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6alEiv7Z7drn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wyAMS5ru7Vfi"
      },
      "outputs": [],
      "source": [
        "# Define a function to plot word embeddings in a 2d space.\n",
        "def plot_embeddings(word_embeddings,vocab=vocab):\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    word_embeddings_2d = tsne.fit_transform(word_embeddings)\n",
        "\n",
        "    # Plotting the results with labels from vocab\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i, word in enumerate(vocab.get_itos() ):  # assuming vocab.itos gives the list of words in your vocab\n",
        "        plt.scatter(word_embeddings_2d[i, 0], word_embeddings_2d[i, 1])\n",
        "        plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]))\n",
        "\n",
        "    plt.xlabel(\"t-SNE component 1\")\n",
        "    plt.ylabel(\"t-SNE component 2\")\n",
        "    plt.title(\"Word Embeddings visualized with t-SNE\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function returns the most similar words to a target word by calculating word vectors' cosine distance\n",
        "def find_similar_words(word, word_embeddings, top_k=5):\n",
        "    if word not in word_embeddings:\n",
        "        print(\"Word not found in embeddings.\")\n",
        "        return []\n",
        "\n",
        "    # Get the embedding for the given word\n",
        "    target_embedding = word_embeddings[word]\n",
        "\n",
        "    # Calculate cosine similarities between the target word and all other words\n",
        "    similarities = {}\n",
        "    for w, embedding in word_embeddings.items():\n",
        "        if w != word:\n",
        "            similarity = torch.dot(target_embedding, embedding) / (\n",
        "                torch.norm(target_embedding) * torch.norm(embedding)\n",
        "            )\n",
        "            similarities[w] = similarity.item()\n",
        "\n",
        "    # Sort the similarities in descending order\n",
        "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the top k similar words\n",
        "    most_similar_words = [w for w, _ in sorted_similarities[:top_k]]\n",
        "    return most_similar_words"
      ],
      "metadata": {
        "id": "y89TKWt27YG5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that trains word2vec model on toy data\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):\n",
        "    \"\"\"\n",
        "    Train the model for the specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to be trained.\n",
        "        dataloader: DataLoader providing data for training.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimizer for updating model's weights.\n",
        "        num_epochs: Number of epochs to train the model for.\n",
        "\n",
        "    Returns:\n",
        "        model: The trained model.\n",
        "        epoch_losses: List of average losses for each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    # List to store running loss for each epoch\n",
        "    epoch_losses = []\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        # Storing running loss values for the current epoch\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Using tqdm for a progress bar\n",
        "        for idx, samples in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Check for EmbeddingBag layer in the model\n",
        "            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):\n",
        "                target, context, offsets = samples\n",
        "                predicted = model(context, offsets)\n",
        "\n",
        "            # Check for Embedding layer in the model\n",
        "            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):\n",
        "                target, context = samples\n",
        "                predicted = model(context)\n",
        "\n",
        "            loss = criterion(predicted, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Append average loss for the epoch\n",
        "        epoch_losses.append(running_loss / len(dataloader))\n",
        "\n",
        "    return model, epoch_losses"
      ],
      "metadata": {
        "id": "chIjwGl78jW-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and train word2vec models"
      ],
      "metadata": {
        "id": "M0NnLq5Y82d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_data = \"\"\"I wish I was little bit taller\n",
        "I wish I was a baller\n",
        "She wore a small black dress to the party\n",
        "The dog chased a big red ball in the park\n",
        "He had a huge smile on his face when he won the race\n",
        "The tiny kitten played with a fluffy toy mouse\n",
        "The team celebrated their victory with a grand parade\n",
        "She bought a small, delicate necklace for her sister\n",
        "The mountain peak stood majestic and tall against the clear blue sky\n",
        "The toddler took small, careful steps as she learned to walk\n",
        "The house had a spacious backyard with a big swimming pool\n",
        "He felt a sense of accomplishment after completing the challenging puzzle\n",
        "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
        "The children played happily in the small, cozy room\n",
        "The book had an enormous impact on readers around the world\n",
        "The wind blew gently, rustling the leaves of the tall trees\n",
        "She painted a beautiful, intricate design on the small canvas\n",
        "The concert hall was filled with thousands of excited fans\n",
        "The garden was adorned with colorful flowers of all sizes\n",
        "I hope to achieve great success in my chosen career path\n",
        "The skyscraper towered above the city, casting a long shadow\n",
        "He gazed in awe at the breathtaking view from the mountaintop\n",
        "The artist created a stunning masterpiece with bold brushstrokes\n",
        "The baby took her first steps, a small milestone that brought joy to her parents\n",
        "The team put in a tremendous amount of effort to win the championship\n",
        "The sun set behind the horizon, painting the sky in vibrant colors\n",
        "The professor gave a fascinating lecture on the history of ancient civilizations\n",
        "The house was filled with laughter and the sound of children playing\n",
        "She received a warm, enthusiastic welcome from the audience\n",
        "The marathon runner had incredible endurance and determination\n",
        "The child's eyes sparkled with excitement upon opening the gift\n",
        "The ship sailed across the vast ocean, guided by the stars\n",
        "The company achieved remarkable growth in a short period of time\n",
        "The team worked together harmoniously to complete the project\n",
        "The puppy wagged its tail, expressing its happiness and affection\n",
        "She wore a stunning gown that made her feel like a princess\n",
        "The building had a grand entrance with towering columns\n",
        "The concert was a roaring success, with the crowd cheering and clapping\n",
        "The baby took a tiny bite of the sweet, juicy fruit\n",
        "The athlete broke a new record, achieving a significant milestone in her career\n",
        "The sculpture was a masterpiece of intricate details and craftsmanship\n",
        "The forest was filled with towering trees, creating a sense of serenity\n",
        "The children built a small sandcastle on the beach, their imaginations running wild\n",
        "The mountain range stretched as far as the eye could see, majestic and awe-inspiring\n",
        "The artist's brush glided smoothly across the canvas, creating a beautiful painting\n",
        "She received a small token of appreciation for her hard work and dedication\n",
        "The orchestra played a magnificent symphony that moved the audience to tears\n",
        "The flower bloomed in vibrant colors, attracting butterflies and bees\n",
        "The team celebrated their victory with a big, extravagant party\n",
        "The child's laughter echoed through the small room, filling it with joy\n",
        "The sunflower stood tall, reaching for the sky with its bright yellow petals\n",
        "The city skyline was dominated by tall buildings and skyscrapers\n",
        "The cake was adorned with a beautiful, elaborate design for the special occasion\n",
        "The storm brought heavy rain and strong winds, causing widespread damage\n",
        "The small boat sailed peacefully on the calm, glassy lake\n",
        "The artist used bold strokes of color to create a striking and vivid painting\n",
        "The couple shared a passionate kiss under the starry night sky\n",
        "The mountain climber reached the summit after a long and arduous journey\n",
        "The child's eyes widened in amazement as the magician performed his tricks\n",
        "The garden was filled with the sweet fragrance of blooming flowers\n",
        "The basketball player made a big jump and scored a spectacular slam dunk\n",
        "The cat pounced on a small mouse, displaying its hunting instincts\n",
        "The mansion had a grand entrance with a sweeping staircase and chandeliers\n",
        "The raindrops fell gently, creating a rhythmic patter on the roof\n",
        "The baby took a big step forward, encouraged by her parents' applause\n",
        "The actor delivered a powerful and emotional performance on stage\n",
        "The butterfly fluttered its delicate wings, mesmerizing those who watched\n",
        "The company launched a small-scale advertising campaign to test the market\n",
        "The building was constructed with strong, sturdy materials to withstand earthquakes\n",
        "The singer's voice was powerful and resonated throughout the concert hall\n",
        "The child built a massive sandcastle with towers, moats, and bridges\n",
        "The garden was teeming with a variety of small insects and buzzing bees\n",
        "The athlete's muscles were well-developed and strong from years of training\n",
        "The sun cast long shadows as it set behind the mountains\n",
        "The couple exchanged heartfelt vows in a beautiful, intimate ceremony\n",
        "The dog wagged its tail vigorously, a sign of excitement and happiness\n",
        "The baby let out a tiny giggle, bringing joy to everyone around\"\"\""
      ],
      "metadata": {
        "id": "vyzDkrZT8qNm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data by tokenizing it and creating a vocabulary from data\n",
        "\n",
        "# Step 1: Get tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')  # This uses basic English tokenizer. You can choose another.\n",
        "\n",
        "# Step 2: Tokenize sentences\n",
        "def tokenize_data(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield tokenizer(sentence)\n",
        "\n",
        "tokenized_toy_data = tokenizer(toy_data)\n",
        "\n",
        "vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ],
      "metadata": {
        "id": "Q862P_JJ85XI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check sentence after tokenization and numericalization\n",
        "\n",
        "# Test\n",
        "sample_sentence = \"I wish I was a baller\"\n",
        "tokenized_sample = tokenizer(sample_sentence)\n",
        "encoded_sample = [vocab[token] for token in tokenized_sample]\n",
        "print(\"Encoded sample:\", encoded_sample)"
      ],
      "metadata": {
        "id": "CL1r6B109ORo",
        "outputId": "6f73fa48-11e0-49c0-b80c-b814c13744d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded sample: [20, 108, 20, 7, 2, 133]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fuction to apply numericalization on all tokens\n",
        "\n",
        "text_pipeline = lambda tokens:[ vocab[token]  for token in tokens]"
      ],
      "metadata": {
        "id": "UfH7WaLo9eZL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag of Words (CBOW)"
      ],
      "metadata": {
        "id": "gFiypt78930A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# context: set of surrounding words (e.g. context window of size 2 means you take two words before and two words after the target word as context)\n",
        "\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "cobow_data = []\n",
        "\n",
        "for i in range(CONTEXT_SIZE, len(tokenized_toy_data ) - CONTEXT_SIZE):\n",
        "\n",
        "    context = (\n",
        "        [tokenized_toy_data [i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)]\n",
        "        + [tokenized_toy_data [i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "\n",
        "    target = tokenized_toy_data [i]\n",
        "    cobow_data.append((context, target))\n",
        "\n"
      ],
      "metadata": {
        "id": "BexTEt_29m7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}