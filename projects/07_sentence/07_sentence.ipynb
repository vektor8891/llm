{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/07_sentence/07_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.15.1\n",
        "# !pip install portalocker\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "qq0keADLNuTR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to RNNs"
      ],
      "metadata": {
        "id": "oYECu1QOLk9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MFdQXsFqJOzD"
      },
      "outputs": [],
      "source": [
        "# State machine (transition model) with 3 states:\n",
        "# 1. State h = -1:\n",
        "#   - Maintains itself when x = 1\n",
        "#   - Proceeds to the h = 0 state if x = -1\n",
        "# 2. State h = 0:\n",
        "#   - Moves to h = -1 state when x = 1\n",
        "#   - Advances to the h = 1 when x = -1\n",
        "# 3. State h = 1:\n",
        "#   - Sustains its position when x = -1\n",
        "#   - Transitions to h = 0 state when x = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "W_xh=torch.tensor(-10.0)\n",
        "W_hh=torch.tensor(10.0)\n",
        "b_h=torch.tensor(0.0)\n",
        "x_t=1\n",
        "h_prev=torch.tensor(-1)\n",
        "\n",
        "X=[1,1,-1,-1,1,1]\n",
        "H=[-1,-1,0,1,0,-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ0jN1TiLG-2",
        "outputId": "41201be2-9b74-4089-e291-780ecc56fc70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-eaec925ceedc>\", line 3, in <cell line: 0>\n",
            "    W_xh=torch.tensor(-10.0)\n",
            "<ipython-input-3-eaec925ceedc>:3: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  W_xh=torch.tensor(-10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store the predicted state values\n",
        "H_hat = []\n",
        "# Loop through each data point in the input sequence X\n",
        "t=1\n",
        "for x in X:\n",
        "    # Assign the current data point to x_t\n",
        "    print(\"t=\",t)\n",
        "    x_t = x\n",
        "    # Print the value of the previous state (h at time t-1)\n",
        "    print(\"h_t-1\", h_prev.item())\n",
        "\n",
        "    # Compute the current state (h at time t) using the RNN formula with tanh activation\n",
        "    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n",
        "\n",
        "    # Update h_prev to the current state value for the next iteration\n",
        "    h_prev = h_t\n",
        "\n",
        "    # Print the current input value (x at time t)\n",
        "    print(\"x_t\", x_t)\n",
        "\n",
        "    # Print the computed state value (h at time t)\n",
        "    print(\"h_t\", h_t.item())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Append the current state value to the H_hat list after converting it to integer\n",
        "    H_hat.append(int(h_t.item()))\n",
        "    t+=1"
      ],
      "metadata": {
        "id": "94AnYIL9LTtD",
        "outputId": "2fafa095-0190-4265-b94f-90f51a1da639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t= 1\n",
            "h_t-1 -1\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n",
            "t= 2\n",
            "h_t-1 -1.0\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n",
            "t= 3\n",
            "h_t-1 -1.0\n",
            "x_t -1\n",
            "h_t 0.0\n",
            "\n",
            "\n",
            "t= 4\n",
            "h_t-1 0.0\n",
            "x_t -1\n",
            "h_t 1.0\n",
            "\n",
            "\n",
            "t= 5\n",
            "h_t-1 1.0\n",
            "x_t 1\n",
            "h_t 0.0\n",
            "\n",
            "\n",
            "t= 6\n",
            "h_t-1 0.0\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H_hat"
      ],
      "metadata": {
        "id": "X8o4OpiALbn_",
        "outputId": "fe9a5934-c8c4-4440-ecf4-c6078a8a38c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, -1, 0, 1, 0, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H"
      ],
      "metadata": {
        "id": "x7uxuEXRLcBr",
        "outputId": "493a8cde-bb4c-43cc-ec10-fe085ea8eeb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, -1, 0, 1, 0, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence-to-sequence architecture\n",
        "\n",
        "## Encoder implementation in PyTorch"
      ],
      "metadata": {
        "id": "Oduh3fbrLpyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "5inDlii6MCti"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        #input_batch = [src len, batch size]\n",
        "        embed = self.dropout(self.embedding(input_batch))\n",
        "        embed = embed.to(device)\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        outputs, (hidden, cell) = self.lstm(embed)\n",
        "\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "4nduj7-wLdgH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_len = 8\n",
        "emb_dim = 10\n",
        "hid_dim=8\n",
        "n_layers=1\n",
        "dropout_prob=0.5\n",
        "\n",
        "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
      ],
      "metadata": {
        "id": "Tbah1KqpL9K0",
        "outputId": "6ff29bd0-30ca-4d3e-c207-57e0942bbf54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_batch = torch.tensor([[0,3,4,2,1]])\n",
        "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
        "src_batch = src_batch.t().to(device)\n",
        "print(\"Shape of input(src) tensor:\", src_batch.shape)\n",
        "hidden_t , cell_t = encoder_t(src_batch)\n",
        "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
      ],
      "metadata": {
        "id": "tUz2HAn5MJ2w",
        "outputId": "b34e4c44-721c-4539-e46d-47c3feb1dc0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input(src) tensor: torch.Size([5, 1])\n",
            "Hidden tensor from encoder: tensor([[[-0.2216,  0.1216,  0.0173,  0.4893, -0.0186, -0.1501, -0.0291,\n",
            "          -0.3514]]], grad_fn=<StackBackward0>) \n",
            "Cell tensor from encoder: tensor([[[-0.4535,  0.2724,  0.0312,  0.6267, -0.0222, -0.2210, -0.0727,\n",
            "          -0.4911]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder implementation in PyTorch"
      ],
      "metadata": {
        "id": "Q0dGXxhTMmSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "\n",
        "        #input = [batch size]\n",
        "\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        prediction_logit = self.fc_out(output.squeeze(0))\n",
        "        prediction = self.softmax(prediction_logit)\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "P_drcRsTMGxL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 6\n",
        "emb_dim=10\n",
        "hid_dim = 8\n",
        "n_layers=1\n",
        "dropout=0.5\n",
        "\n",
        "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
      ],
      "metadata": {
        "id": "tEyj1IgMMuFI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_t = torch.tensor([0]).to(device) #<bos>\n",
        "input_t.shape\n",
        "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
        "print(\"Prediction:\", prediction, '\\nHidden:',hidden,'\\nCell:', cell)"
      ],
      "metadata": {
        "id": "s-L0MHZcMxQa",
        "outputId": "d79bb41f-d61a-4f0c-850b-c26593933ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[-1.9059, -2.0818, -2.0957, -2.0935, -1.3157, -1.5507]],\n",
            "       grad_fn=<LogSoftmaxBackward0>) \n",
            "Hidden: tensor([[[-0.0506, -0.2892, -0.0307,  0.2635, -0.1041, -0.0849, -0.1230,\n",
            "          -0.1663]]], grad_fn=<StackBackward0>) \n",
            "Cell: tensor([[[-0.0722, -0.4795, -0.0653,  0.5333, -0.2209, -0.1418, -0.2082,\n",
            "          -0.5806]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-decoder connection"
      ],
      "metadata": {
        "id": "VJJcf9sfNAYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "#trg = [trg len, batch size]\n",
        "#teacher_forcing_ratio is probability to use teacher forcing\n",
        "#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
        "teacher_forcing_ratio = 0.5\n",
        "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
        "\n",
        "\n",
        "batch_size = trg.shape[1]\n",
        "trg_len = trg.shape[0]\n",
        "trg_vocab_size = decoder_t.output_dim\n",
        "\n",
        "#tensor to store decoder outputs\n",
        "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
        "\n",
        "#send to device\n",
        "\n",
        "hidden_t = hidden_t.to(device)\n",
        "cell_t = cell_t.to(device)\n",
        "\n",
        "\n",
        "#first input to the decoder is the <bos> tokens\n",
        "input = trg[0,:]\n",
        "\n",
        "\n",
        "for t in range(1, trg_len):\n",
        "\n",
        "    #you loop through the trg len and generate tokens\n",
        "    #decoder receives previous generated token, cell and hidden\n",
        "    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n",
        "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
        "\n",
        "    #place predictions in a tensor holding predictions for each token\n",
        "    outputs_t[t] = output_t\n",
        "\n",
        "    #decide if you are going to use teacher forcing or not\n",
        "    teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "    #get the highest predicted token from your predictions\n",
        "    top1 = output_t.argmax(1)\n",
        "\n",
        "\n",
        "    #if teacher forcing, use actual next token as next input\n",
        "    #if not, use predicted token\n",
        "    #input = trg[t] if teacher_force else top1\n",
        "    input = trg[t] if teacher_force else top1\n",
        "\n",
        "print(outputs_t,outputs_t.shape )"
      ],
      "metadata": {
        "id": "L2BXmY6XMzsU",
        "outputId": "6fa0ecb2-25d5-4b6c-e8f0-9ad3ba0749ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.9284, -2.0987, -2.0450, -2.0590, -1.3741, -1.5053]],\n",
            "\n",
            "        [[-1.7378, -1.8598, -2.2121, -1.9482, -1.6806, -1.4691]],\n",
            "\n",
            "        [[-1.9278, -2.0284, -2.1986, -1.8809, -1.5506, -1.3965]],\n",
            "\n",
            "        [[-1.9514, -2.0791, -2.1704, -1.9349, -1.4625, -1.4161]]],\n",
            "       grad_fn=<CopySlices>) torch.Size([5, 1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n",
        "pred_tokens = outputs_t.argmax(2)\n",
        "print(pred_tokens)\n"
      ],
      "metadata": {
        "id": "F9da6-HvM2oN",
        "outputId": "a8f7d464-eca9-4cd5-ed04-1ce63ae7e379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0],\n",
            "        [4],\n",
            "        [5],\n",
            "        [5],\n",
            "        [5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence-to-sequence model implementation in PyTorch"
      ],
      "metadata": {
        "id": "mRaD0DgoNPqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device,trg_vocab):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
        "\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        hidden = hidden.to(device)\n",
        "        cell = cell.to(device)\n",
        "\n",
        "\n",
        "        #first input to the decoder is the <bos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if you are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from your predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            #input = trg[t] if teacher_force else top1\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Nw0e-zPwM3y8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model in PyTorch"
      ],
      "metadata": {
        "id": "cJooxGKqNXd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Wrap iterator with tqdm for progress logging\n",
        "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (src,trg) in enumerate(iterator):\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "\n",
        "        trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tqdm progress bar with the current loss\n",
        "        train_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    return epoch_loss / len(list(iterator))\n"
      ],
      "metadata": {
        "id": "JRgSDrziM4EL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating model in PyTorch"
      ],
      "metadata": {
        "id": "izsKISeGNcor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Wrap iterator with tqdm for progress logging\n",
        "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (src,trg) in enumerate(iterator):\n",
        "\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "\n",
        "            trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            # Update tqdm progress bar with the current loss\n",
        "            valid_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(list(iterator))\n"
      ],
      "metadata": {
        "id": "y0ITnn-kM4cb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "mJmxGwQWNgtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"
      ],
      "metadata": {
        "id": "xZeIvpWNM4vE",
        "outputId": "68f67bac-0e90-454f-a680-1ac4871dce1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 15:40:23--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4890 (4.8K) [application/x-python]\n",
            "Saving to: ‘Multi30K_de_en_dataloader.py.1’\n",
            "\n",
            "Multi30K_de_en_data 100%[===================>]   4.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-24 15:40:23 (1.09 GB/s) - ‘Multi30K_de_en_dataloader.py.1’ saved [4890/4890]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run Multi30K_de_en_dataloader.py"
      ],
      "metadata": {
        "id": "DdFKvSy4M5JI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)\n",
        "src, trg = next(iter(train_dataloader))\n",
        "src,trg"
      ],
      "metadata": {
        "id": "NK9ttbsSM5jU",
        "outputId": "56d0d6d1-274d-433e-e7be-67a71931bc7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Multi30K_de_en_dataloader.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
            "/content/Multi30K_de_en_dataloader.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    2,     2,     2,     2],\n",
              "         [    3,  5510,  5510, 12642],\n",
              "         [    1,     3,     3,     8],\n",
              "         [    1,     1,     1,  1701],\n",
              "         [    1,     1,     1,     3]]),\n",
              " tensor([[   2,    2,    2,    2],\n",
              "         [   3, 6650,  216,    6],\n",
              "         [   1, 4623,  110, 3398],\n",
              "         [   1,  259, 3913,  202],\n",
              "         [   1,  172, 1650,  109],\n",
              "         [   1, 9953, 3823,   37],\n",
              "         [   1,  115,   71,    3],\n",
              "         [   1,  692, 2808,    1],\n",
              "         [   1, 3428, 2187,    1],\n",
              "         [   1,    5,    5,    1],\n",
              "         [   1,    3,    3,    1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_itr = iter(train_dataloader)\n",
        "# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n",
        "for n in range(1000):\n",
        "    german, english= next(data_itr)\n",
        "\n",
        "for n in range(3):\n",
        "    german, english=next(data_itr)\n",
        "    german=german.T\n",
        "    english=english.T\n",
        "    print(\"________________\")\n",
        "    print(\"german\")\n",
        "    for g in german:\n",
        "        print(index_to_german(g))\n",
        "    print(\"________________\")\n",
        "    print(\"english\")\n",
        "    for e in english:\n",
        "        print(index_to_eng(e))"
      ],
      "metadata": {
        "id": "tRbRvxXlM5zM",
        "outputId": "a0a7f41e-d728-4796-ee92-5635d31923ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________\n",
            "german\n",
            "<bos> Personen mit schwarzen Hüten in der Innenstadt . <eos>\n",
            "<bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>\n",
            "<bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>\n",
            "<bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> People in black hats gathered together downtown . <eos> <pad> <pad> <pad>\n",
            "<bos> A group of people protesting in a city . <eos> <pad> <pad>\n",
            "<bos> A group is letting their political opinion be known . <eos> <pad>\n",
            "<bos> A group of people are sitting on a rocky beach . <eos>\n",
            "________________\n",
            "german\n",
            "<bos> Zwei sitzende Personen mit Hüten und Sonnenbrillen . <eos>\n",
            "<bos> Ein kleiner Junge mit Hut beim Angeln . <eos>\n",
            "<bos> Diese zwei Frauen haben Spaß im Giorgio's . <eos>\n",
            "<bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>\n",
            "<bos> A young boy in a hat is fishing by himself . <eos>\n",
            "<bos> These two women is at Giorgio 's having fun . <eos> <pad>\n",
            "<bos> Two young children are asleep on a couch . <eos> <pad> <pad>\n",
            "________________\n",
            "german\n",
            "<bos> Zwei junge Mädchen marschieren in einem Umzug . <eos>\n",
            "<bos> Eine Frau läuft vor einer gestreiften Wand . <eos>\n",
            "<bos> Ein Mann fährt Jet-Ski auf dem Ozean . <eos>\n",
            "<bos> Die städtischen Straßenbahnen an einem sonnigen Tag . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad> <pad>\n",
            "<bos> A woman is running in front of a striped wall . <eos> <pad>\n",
            "<bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>\n",
            "<bos> The urban trolly 's of a city on a sunny day . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "e_g23TxHQOY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "vMyQKnoMM5-T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_transform['de'])\n",
        "OUTPUT_DIM = len(vocab_transform['en'])\n",
        "ENC_EMB_DIM = 128 #256\n",
        "DEC_EMB_DIM = 128 #256\n",
        "HID_DIM = 256 #512\n",
        "N_LAYERS = 1 #2\n",
        "ENC_DROPOUT = 0.3 #0.5\n",
        "DEC_DROPOUT = 0.3 #0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
      ],
      "metadata": {
        "id": "2xsvzv_VM6KA",
        "outputId": "adcaab69-445a-4a1e-dcbc-d6454629c2a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "P58zS6MnM6VR",
        "outputId": "baadad66-1e96-4b2f-86b4-520cb4696ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19214, 128)\n",
              "    (lstm): LSTM(128, 256, dropout=0.3)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10837, 128)\n",
              "    (lstm): LSTM(128, 256, dropout=0.3)\n",
              "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
              "    (softmax): LogSoftmax(dim=1)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (trg_vocab): Vocab()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "kBsYv78lM6gt",
        "outputId": "037173d5-0f7b-4c0c-e358-e4d15440785e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 7,422,165 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "9-jxnpZ3M6th"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training epochs (in `CUDA` environment)"
      ],
      "metadata": {
        "id": "TV6QodrF_fzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# from tqdm import tqdm\n",
        "# import math\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# N_EPOCHS = 3 #run the training for at least 5 epochs\n",
        "# CLIP = 1\n",
        "\n",
        "# best_valid_loss = float('inf')\n",
        "# best_train_loss = float('inf')\n",
        "# train_losses = []\n",
        "# valid_losses = []\n",
        "\n",
        "# train_PPLs = []\n",
        "# valid_PPLs = []\n",
        "\n",
        "# for epoch in range(N_EPOCHS):\n",
        "\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "#     train_ppl = math.exp(train_loss)\n",
        "#     valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "#     valid_ppl = math.exp(valid_loss)\n",
        "\n",
        "\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "\n",
        "#     if valid_loss < best_valid_loss:\n",
        "\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(model.state_dict(), 'RNN-TR-model.pt')\n",
        "\n",
        "#     train_losses.append(train_loss)\n",
        "#     train_PPLs.append(train_ppl)\n",
        "#     valid_losses.append(valid_loss)\n",
        "#     valid_PPLs.append(valid_ppl)\n",
        "\n",
        "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
        "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')"
      ],
      "metadata": {
        "id": "6IWSSO0zM64d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Create a list of epoch numbers\n",
        "# epochs = [epoch+1 for epoch in range(N_EPOCHS)]\n",
        "\n",
        "# # Create the figure and axes\n",
        "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "# ax2 = ax1.twinx()\n",
        "\n",
        "# # Plotting the training and validation loss\n",
        "# ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "# ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')\n",
        "# ax1.set_xlabel('Epochs')\n",
        "# ax1.set_ylabel('Loss')\n",
        "# ax1.set_title('Training and Validation Loss/PPL')\n",
        "\n",
        "# # Plotting the training and validation perplexity\n",
        "# ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')\n",
        "# ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')\n",
        "# ax2.set_ylabel('Perplexity')\n",
        "\n",
        "# # Adjust the y-axis scaling for PPL plot\n",
        "# ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)\n",
        "\n",
        "# # Set the legend\n",
        "# lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "# lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "# lines = lines1 + lines2\n",
        "# labels = labels1 + labels2\n",
        "# ax1.legend(lines, labels, loc='upper right')\n",
        "\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "z9aBFAaHM7ET"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the saved model"
      ],
      "metadata": {
        "id": "pOKcDsXM_pKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'\n",
        "model.load_state_dict(torch.load('RNN-TR-model.pt',map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "gRL4Nfwq92UV",
        "outputId": "0245d8b0-988b-49f9-aa11-52d522a359f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 16:31:36--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29692282 (28M) [binary/octet-stream]\n",
            "Saving to: ‘RNN-TR-model.pt’\n",
            "\n",
            "RNN-TR-model.pt     100%[===================>]  28.32M  52.8MB/s    in 0.5s    \n",
            "\n",
            "2025-03-24 16:31:37 (52.8 MB/s) - ‘RNN-TR-model.pt’ saved [29692282/29692282]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model inference"
      ],
      "metadata": {
        "id": "SkczoGtF_vj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Create a generator function that generates translations for input source sentences\n",
        "def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)\n",
        "\n",
        "        # Pass the source tensor through the encoder\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # Create a tensor to store the generated translation\n",
        "        # get_stoi() maps tokens to indices\n",
        "        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token\n",
        "\n",
        "        # Convert the initial token to a PyTorch tensor\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "        # Move the tensor to the same device as the model\n",
        "        trg_tensor = trg_tensor.to(model.device)\n",
        "\n",
        "\n",
        "        # Generate the translation\n",
        "        for _ in range(max_len):\n",
        "\n",
        "            # Pass the target tensor and the previous hidden and cell states through the decoder\n",
        "            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)\n",
        "\n",
        "            # Get the predicted next token\n",
        "            pred_token = output.argmax(1)[-1].item()\n",
        "\n",
        "            # Append the predicted token to the translation\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "\n",
        "            # If the predicted token is the <eos> token, stop generating\n",
        "            if pred_token == trg_vocab.get_stoi()['<eos>']:\n",
        "                break\n",
        "\n",
        "            # Convert the predicted token to a PyTorch tensor\n",
        "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "            # Move the tensor to the same device as the model\n",
        "            trg_tensor = trg_tensor.to(model.device)\n",
        "\n",
        "        # Convert the generated tokens to text\n",
        "        # get_itos() maps indices to tokens\n",
        "        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n",
        "\n",
        "        # Remove the <sos> and <eos> from the translation\n",
        "        if trg_tokens[0] == '<bos>':\n",
        "            trg_tokens = trg_tokens[1:]\n",
        "        if trg_tokens[-1] == '<eos>':\n",
        "            trg_tokens = trg_tokens[:-1]\n",
        "\n",
        "        # Return the translation list as a string\n",
        "\n",
        "        translation = \" \".join(trg_tokens)\n",
        "\n",
        "        return translation"
      ],
      "metadata": {
        "id": "aIFC6s2Y_s0D"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the model's output for a sample sentence\n",
        "\n",
        "# model.load_state_dict(torch.load('RNN-TR-model.pt'))\n",
        "\n",
        "# Actual translation: Asian man sweeping the walkway.\n",
        "src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'\n",
        "\n",
        "generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)\n",
        "#generated_translation = \" \".join(generated_translation_list).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "print(generated_translation)"
      ],
      "metadata": {
        "id": "AVVjHRzT_yTu",
        "outputId": "5aedbea6-b350-4881-afdd-95890e0511e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An Asian man is on the sidewalk .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU score metric for evaluation\n",
        "\n",
        "- BLEU (Bilingual Evaluation Understudy): measures similarity between generated translation and reference translations based on n-gram matching\n",
        "- peplexity: evaluate next token <-> BLEU score: evaluate final generated translation\n",
        "\n",
        "### Calculation\n",
        "\n",
        "1. **Precision**:\n",
        "   - Proportion of n-grams in the generated translation that appear in the reference translations.\n",
        "   - Calculated for each n-gram order & combined using geometric mean.\n",
        "   - Formula for single n-gram order:\n",
        "   \n",
        "   $$\\text{Precision}_n(t) = \\frac{\\text{CountClip}_n(t)}{\\text{Count}_n(t)}$$\n",
        "   \n",
        "   where:\n",
        "     - $\\text{CountClip}_n(t)$ is the count of n-grams in the generated translation that appear in any reference translation, clipped by the maximum count of that n-gram in any single reference translation.\n",
        "     - $\\text{Count}_n(t)$ is the count of n-grams in the generated translation.\n",
        "\n",
        "2. . **Brevity penalty**:\n",
        "   - Accounts for the fact that shorter translations tend to have higher precision scores.\n",
        "   - Eencourages translations that are closer in length to the reference translations.\n",
        "   - Formula:\n",
        "   \n",
        "   $$\\text{BP} = \\begin{cases} 1 & \\text{if } c > r \\\\ e^{(1 - \\frac{r}{c})} & \\text{if } c \\leq r \\end{cases}$$\n",
        "   \n",
        "   where:\n",
        "     - $c$ is the total length of the generated translation.\n",
        "     - $r$ is the total length of the reference translations.\n",
        "\n",
        "3. **BLEU score**:\n",
        "   - Geometric mean of the precisions, weighted by the brevity penalty:\n",
        "   \n",
        "   $$\\text{BLEU} = \\text{BP} \\cdot \\exp(\\sum_{n=1}^{N}w_n \\log(\\text{Precision}_n(t)))$$\n",
        "   \n",
        "   where:\n",
        "     - $N$ is the maximum n-gram order.\n",
        "     - $w_n$ is the weight assigned to the precision at n-gram order $n$, commonly set as $\\frac{1}{N}$ for equal weights."
      ],
      "metadata": {
        "id": "w5VUmly-AEps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(generated_translation, reference_translations):\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
        "    references = [reference.split() for reference in reference_translations]\n",
        "    hypothesis = generated_translation.split()\n",
        "\n",
        "    # Calculate the BLEU score\n",
        "    bleu_score = sentence_bleu(references, hypothesis)\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "L9isMGeS__JZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_translations = [\n",
        "    \"Asian man sweeping the walkway .\",\n",
        "    \"An asian man sweeping the walkway .\",\n",
        "    \"An Asian man sweeps the sidewalk .\",\n",
        "    \"An Asian man is sweeping the sidewalk .\",\n",
        "    \"An asian man is sweeping the walkway .\",\n",
        "    \"Asian man sweeping the sidewalk .\"\n",
        "]\n",
        "\n",
        "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "id": "H8B-56HURPi2",
        "outputId": "da216cb6-ff24-450d-f870-00f2e6dfbf46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translate a German sentence to English"
      ],
      "metadata": {
        "id": "G-e7xoe8RZaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "german_text = \"Menschen gehen auf der Straße\"\n",
        "\n",
        "# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.\n",
        "english_translation = generate_translation(\n",
        "    model,\n",
        "    src_sentence=german_text,\n",
        "    src_vocab=vocab_transform['de'],\n",
        "    trg_vocab=vocab_transform['en'],\n",
        "    max_len=50\n",
        ")\n",
        "\n",
        "# Display the original and translated text\n",
        "print(f\"Original German text: {german_text}\")\n",
        "print(f\"Translated English text: {english_translation}\")"
      ],
      "metadata": {
        "id": "4D9CRlvORQ8s",
        "outputId": "4a6c10b5-2f95-480f-b651-567863fd5ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original German text: Menschen gehen auf der Straße\n",
            "Translated English text: People are walking on the street .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IjaNrYoRekX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}