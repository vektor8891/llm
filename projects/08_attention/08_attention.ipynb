{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/08_attention/08_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XR5qsL9LGCCJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "block_size = 16               # Context length for predictions\n",
        "n_embd = 32                   # Embedding size\n",
        "num_heads = 2                 # Number of head in multi-headed attention\n",
        "n_layer = 2                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(my_embdings,name,vocab):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # Plot the data points\n",
        "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
        "\n",
        "  # Label the points\n",
        "  for j, label in enumerate(name):\n",
        "      i=vocab.get_stoi()[label]\n",
        "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
        "\n",
        "  # Set axis labels\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "IDSAfCFeGpYF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program for literal translation"
      ],
      "metadata": {
        "id": "oAIrcom5HKrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "    'le': 'the'\n",
        "    , 'chat': 'cat'\n",
        "    , 'est': 'is'\n",
        "    , 'sous': 'under'\n",
        "    , 'la': 'the'\n",
        "    , 'table': 'table'\n",
        "}"
      ],
      "metadata": {
        "id": "A6ylN6m7HKbf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split a sentence into tokens (words)\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function takes a string of text as input and returns a list of words (tokens).\n",
        "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
        "    \"\"\"\n",
        "    return text.split()  # Split the input text on whitespace and return the list of tokens\n",
        "\n",
        "\n",
        "# Function to find the closest key in the dictionary to the given query word\n",
        "def find_closest_key(query):\n",
        "    from Levenshtein import distance\n",
        "    \"\"\"\n",
        "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
        "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
        "    \"\"\"\n",
        "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
        "    for key in dictionary.keys():\n",
        "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
        "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
        "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
        "    return closest_key  # Return the closest key found\n",
        "\n",
        "# Function to translate a sentence from source to target language using the dictionary\n",
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
        "    It constructs the translated sentence by appending the translated words together.\n",
        "    \"\"\"\n",
        "    out = ''  # Initialize the output string\n",
        "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
        "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
        "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
        "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
      ],
      "metadata": {
        "id": "ODRZ9terHHQq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to neural network"
      ],
      "metadata": {
        "id": "HT_WqLFbHmEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and sort the input vocabulary from the dictionary's keys\n",
        "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
        "# Display the size and the sorted vocabulary for the input language\n",
        "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
        "\n",
        "# Create and sort the output vocabulary from the dictionary's values\n",
        "vocabulary_out = sorted(list(set(dictionary.values())))\n",
        "# Display the size and the sorted vocabulary for the output language\n",
        "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AnQIhGzHk7p",
        "outputId": "b5017459-8c9d-43a4-e13d-cc22d02f7183"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
            "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
        "def encode_one_hot(vocabulary):\n",
        "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
        "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
        "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
        "\n",
        "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
        "    for i, key in enumerate(vocabulary):\n",
        "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
        "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
        "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
        "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
        "\n",
        "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors\n",
        "\n",
        "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
        "one_hot_in = encode_one_hot(vocabulary_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwFIvG3rHO5Q",
        "outputId": "d628cf02-b4cb-4bc8-c6f9-50ca48bc1c17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
            "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
            "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
            "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
            "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
            "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
        "# This visualizes the one-hot representation for each word in the input vocabulary\n",
        "for k, v in one_hot_in.items():\n",
        "    print(f\"E_{{ {k} }} = \" , v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2-BeHW1HzdZ",
        "outputId": "7b701028-4758-4520-d10a-06d8bad7db14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
            "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
            "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
            "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
            "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
            "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
        "# This time we're encoding the target language vocabulary\n",
        "one_hot_out = encode_one_hot(vocabulary_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_rlj_VNH1hI",
        "outputId": "ce9ec26d-8827-4c49-a6b7-6655d68cf45c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\t: tensor([1., 0., 0., 0., 0.])\n",
            "is\t: tensor([0., 1., 0., 0., 0.])\n",
            "table\t: tensor([0., 0., 1., 0., 0.])\n",
            "the\t: tensor([0., 0., 0., 1., 0.])\n",
            "under\t: tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a 'dictionary' using matrix multiplication"
      ],
      "metadata": {
        "id": "Jyfq_4fLIrpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
        "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
        "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
        "\n",
        "# Display the tensor for verification\n",
        "print(K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3zyFoE3H4R_",
        "outputId": "23bb2d9a-f2ec-4c3c-da31-d931474f4f42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
        "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
        "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
        "\n",
        "# Display the tensor for verification\n",
        "print(V)"
      ],
      "metadata": {
        "id": "6CG3SDzyIxKN",
        "outputId": "4171f9d5-39d0-432c-f0d6-b75441bf2562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating how to look up a translation for a given word using matrix operations\n",
        "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
        "q = one_hot_in['sous']\n",
        "# Display the query token vector\n",
        "print(\"Query token :\", q)"
      ],
      "metadata": {
        "id": "JsLWrrovI0dD",
        "outputId": "e044c0d8-8843-4b06-97e9-4fc09713bc03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query token : tensor([0., 0., 0., 0., 1., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the corresponding key vector in K (input dictionary matrix) using matrix multiplication\n",
        "# This operation gives us the index where 'sous' would be '1' in the one-hot encoded input matrix\n",
        "print(\"Select key (K) :\", q @ K.T)"
      ],
      "metadata": {
        "id": "URRuELZCI3f1",
        "outputId": "787fb0b1-007e-4d78-cb17-9deff7fce7b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select key (K) : tensor([0., 0., 0., 1., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enyQYmkUKXZ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}