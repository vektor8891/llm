{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/08_attention/08_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.15.1"
      ],
      "metadata": {
        "id": "0UHHt8XJIq28"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XR5qsL9LGCCJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "block_size = 16               # Context length for predictions\n",
        "n_embd = 32                   # Embedding size\n",
        "num_heads = 2                 # Number of head in multi-headed attention\n",
        "n_layer = 2                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(my_embdings,name,vocab):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # Plot the data points\n",
        "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
        "\n",
        "  # Label the points\n",
        "  for j, label in enumerate(name):\n",
        "      i=vocab.get_stoi()[label]\n",
        "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
        "\n",
        "  # Set axis labels\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "IDSAfCFeGpYF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program for literal translation"
      ],
      "metadata": {
        "id": "oAIrcom5HKrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "    'le': 'the'\n",
        "    , 'chat': 'cat'\n",
        "    , 'est': 'is'\n",
        "    , 'sous': 'under'\n",
        "    , 'la': 'the'\n",
        "    , 'table': 'table'\n",
        "}"
      ],
      "metadata": {
        "id": "A6ylN6m7HKbf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split a sentence into tokens (words)\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function takes a string of text as input and returns a list of words (tokens).\n",
        "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
        "    \"\"\"\n",
        "    return text.split()  # Split the input text on whitespace and return the list of tokens\n",
        "\n",
        "\n",
        "# Function to find the closest key in the dictionary to the given query word\n",
        "def find_closest_key(query):\n",
        "    from Levenshtein import distance\n",
        "    \"\"\"\n",
        "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
        "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
        "    \"\"\"\n",
        "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
        "    for key in dictionary.keys():\n",
        "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
        "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
        "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
        "    return closest_key  # Return the closest key found\n",
        "\n",
        "# Function to translate a sentence from source to target language using the dictionary\n",
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
        "    It constructs the translated sentence by appending the translated words together.\n",
        "    \"\"\"\n",
        "    out = ''  # Initialize the output string\n",
        "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
        "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
        "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
        "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
      ],
      "metadata": {
        "id": "ODRZ9terHHQq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to neural network"
      ],
      "metadata": {
        "id": "HT_WqLFbHmEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and sort the input vocabulary from the dictionary's keys\n",
        "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
        "# Display the size and the sorted vocabulary for the input language\n",
        "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
        "\n",
        "# Create and sort the output vocabulary from the dictionary's values\n",
        "vocabulary_out = sorted(list(set(dictionary.values())))\n",
        "# Display the size and the sorted vocabulary for the output language\n",
        "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AnQIhGzHk7p",
        "outputId": "9ee11378-326f-4a93-c9f0-be8609611a7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
            "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
        "def encode_one_hot(vocabulary):\n",
        "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
        "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
        "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
        "\n",
        "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
        "    for i, key in enumerate(vocabulary):\n",
        "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
        "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
        "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
        "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
        "\n",
        "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors\n",
        "\n",
        "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
        "one_hot_in = encode_one_hot(vocabulary_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwFIvG3rHO5Q",
        "outputId": "a18dfbbf-bfd0-4c9a-f95c-4862f1b61832"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
            "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
            "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
            "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
            "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
            "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
        "# This visualizes the one-hot representation for each word in the input vocabulary\n",
        "for k, v in one_hot_in.items():\n",
        "    print(f\"E_{{ {k} }} = \" , v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2-BeHW1HzdZ",
        "outputId": "27895082-bb72-44b5-b0b7-4346653bb683"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
            "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
            "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
            "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
            "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
            "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
        "# This time we're encoding the target language vocabulary\n",
        "one_hot_out = encode_one_hot(vocabulary_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_rlj_VNH1hI",
        "outputId": "4cfdca36-03e4-44c4-d0a6-8e19e392998a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\t: tensor([1., 0., 0., 0., 0.])\n",
            "is\t: tensor([0., 1., 0., 0., 0.])\n",
            "table\t: tensor([0., 0., 1., 0., 0.])\n",
            "the\t: tensor([0., 0., 0., 1., 0.])\n",
            "under\t: tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a 'dictionary' using matrix multiplication"
      ],
      "metadata": {
        "id": "Jyfq_4fLIrpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
        "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
        "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
        "\n",
        "# Display the tensor for verification\n",
        "print(K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3zyFoE3H4R_",
        "outputId": "7d81a02c-f887-496a-9161-488a3b564536"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
        "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
        "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
        "\n",
        "# Display the tensor for verification\n",
        "print(V)"
      ],
      "metadata": {
        "id": "6CG3SDzyIxKN",
        "outputId": "7d50042e-b253-47bb-eac9-74ce18df3f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating how to look up a translation for a given word using matrix operations\n",
        "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
        "q = one_hot_in['sous']\n",
        "# Display the query token vector\n",
        "print(\"Query token :\", q)"
      ],
      "metadata": {
        "id": "JsLWrrovI0dD",
        "outputId": "2e53082e-97cc-4584-f3ae-5c0d30e7f141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query token : tensor([0., 0., 0., 0., 1., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the corresponding key vector in K (input dictionary matrix) using matrix multiplication\n",
        "# This operation gives us the index where 'sous' would be '1' in the one-hot encoded input matrix\n",
        "print(\"Select key (K) :\", q @ K.T)"
      ],
      "metadata": {
        "id": "URRuELZCI3f1",
        "outputId": "48af76d8-b476-4098-c7b6-69a9965535f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select key (K) : tensor([0., 0., 0., 1., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n",
        "# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n",
        "print(\"Select value (V):\", q @ K.T @ V)\n",
        "\n",
        "# The final output demonstrates how 'sous' can be translated using the neural network approach"
      ],
      "metadata": {
        "id": "enyQYmkUKXZ_",
        "outputId": "050a0e02-414b-4055-a84a-132c700bcad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select value (V): tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decode one-hot vector"
      ],
      "metadata": {
        "id": "Glu5VzRHModj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_one_hot(one_hot, vector):\n",
        "    \"\"\"\n",
        "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
        "    \"\"\"\n",
        "    best_key, best_cosine_sim = None, 0\n",
        "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
        "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
        "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
        "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
        "    return best_key  # Return the token corresponding to the one-hot vector"
      ],
      "metadata": {
        "id": "-0dR4q8eMfKE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix-based translate function"
      ],
      "metadata": {
        "id": "XMpti-StMtUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
        "    \"\"\"\n",
        "    sentence_out = ''  # Initialize the output sentence\n",
        "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
        "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
        "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
        "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
        "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
        "    return sentence_out.strip()  # Return the translated sentence"
      ],
      "metadata": {
        "id": "Edkvx3ksMs33"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation test"
      ],
      "metadata": {
        "id": "WdWee3JNMyXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"le chat est sous la table\")"
      ],
      "metadata": {
        "id": "vQ34dSt6Mq2F",
        "outputId": "52f3ad49-c033-45bb-e6d5-ea98db77ce0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax function for similarity"
      ],
      "metadata": {
        "id": "wWemS1m_M78j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('E_{table} = ', one_hot_in['table'])"
      ],
      "metadata": {
        "id": "DLRZI7QGM4tQ",
        "outputId": "921a6282-976c-4993-e8bc-6f65dde993bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_{table} =  tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation with attention mechanism"
      ],
      "metadata": {
        "id": "kJ7aRZwTNFtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    Translate a sentence using the attention mechanism represented by the K and V matrices.\n",
        "    The softmax function is used to calculate a weighted sum of the V vectors, focusing on the most relevant vector for translation.\n",
        "    \"\"\"\n",
        "    sentence_out = ''  # Initialize the output sentence\n",
        "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
        "        q = one_hot_in[token_in]  # Get the one-hot vector for the current token\n",
        "        # Apply softmax to the scaled dot product of q and K.T, then multiply by V\n",
        "        # This selects the most relevant translation vector from V\n",
        "        out = torch.softmax(q @ K.T, dim=0) @ V\n",
        "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output vector to a token\n",
        "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
        "    return sentence_out.strip()  # Return the translated sentence\n",
        "\n",
        "# Test the translate function\n",
        "translate(\"le chat est sous la table\")"
      ],
      "metadata": {
        "id": "eZy5OMprM9lb",
        "outputId": "9ff4b926-ae8b-48b1-ee21-0a532b741cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the 'Q' matrix"
      ],
      "metadata": {
        "id": "dcVzCDWTNXVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The sentence we want to translate\n",
        "sentence = \"le chat est sous la table\"\n",
        "\n",
        "# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q matrix\n",
        "Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
        "\n",
        "# Display the Q matrix\n",
        "print(Q)"
      ],
      "metadata": {
        "id": "BopvZHQYNHpD",
        "outputId": "dcd5de17-0436-499d-9b3e-97a8401e2016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated translate function"
      ],
      "metadata": {
        "id": "1I0Kl1sNNqAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    Translate a sentence using matrix multiplication in parallel.\n",
        "    This function replaces the iterative approach with a single matrix multiplication step,\n",
        "    applying the attention mechanism across all tokens at once.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentence and stack the one-hot vectors to form the Q matrix\n",
        "    Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
        "\n",
        "    # Apply softmax to the dot product of Q and K.T and multiply by V\n",
        "    # This will give us the output vectors for all tokens in parallel\n",
        "    out = torch.softmax(Q @ K.T, 0) @ V\n",
        "\n",
        "    # Decode each one-hot vector in the output to the corresponding token\n",
        "    # And join the tokens to form the translated sentence\n",
        "    return ' '.join([decode_one_hot(one_hot_out, o) for o in out])\n",
        "\n",
        "# Test the function to ensure it produces the correct translation\n",
        "translate(\"le chat est sous la table\")"
      ],
      "metadata": {
        "id": "qdwPZC8vNZHV",
        "outputId": "babdf2e1-598c-4a39-c3fe-a8980e04bcd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention class (optional)"
      ],
      "metadata": {
        "id": "HfiO7B9hNuMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Self attention head. This class implements a self-attention mechanism\n",
        "        which is a key component of transformer-based neural network architectures. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Initialize the superclass (nn.Module)\n",
        "        # Embedding layer to convert input token indices to vectors of fixed size (n_embd)\n",
        "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        # Linear layers to compute the queries, keys, and values from the embeddings\n",
        "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "    def attention(self, x):\n",
        "        embedded_x = self.embedding(x)\n",
        "        k = self.key(embedded_x)\n",
        "        q = self.query(embedded_x)\n",
        "        v = self.value(embedded_x)\n",
        "        # Attention score\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "        return embedded_x,k,q,v,w\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_x = self.embedding(x)\n",
        "        k = self.key(embedded_x)\n",
        "        q = self.query(embedded_x)\n",
        "        v = self.value(embedded_x)\n",
        "        # Attention score\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "        # Add weighted values\n",
        "        out = w @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "Od5heykmNrb-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset definition"
      ],
      "metadata": {
        "id": "Ld5AaBgJOCeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
        "    (3,\"he painted the car red\"),\n",
        "    (1,\"he painted the red car\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "Ipj3zNd0N6-5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization setup"
      ],
      "metadata": {
        "id": "ZhDEoZ6oOG2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")  # Get a basic English tokenizer"
      ],
      "metadata": {
        "id": "RcXa63oeOE2e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary building"
      ],
      "metadata": {
        "id": "aqxFxAs5OSDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    \"\"\"Yield list of tokens in the dataset.\"\"\"\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown words"
      ],
      "metadata": {
        "id": "C1P7n4IMOIu9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text processing pipeline"
      ],
      "metadata": {
        "id": "kOFXbgBkOyGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x):\n",
        "    \"\"\"Converts a text string to a list of token indices.\"\"\"\n",
        "    return vocab(tokenizer(x))  # Tokenize the input and map each token to its index in the vocabulary"
      ],
      "metadata": {
        "id": "lhdON6cXOWIX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter definition"
      ],
      "metadata": {
        "id": "YMV0YGfWO18i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)  # Total number of tokens in the vocabulary\n",
        "n_embd = 3  # Dimension of the embedding space\n",
        "\n",
        "# Create the attention head with the integrated embedding layer\n",
        "attention_head = Head()"
      ],
      "metadata": {
        "id": "bs24-z6mOzzF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dummy data for testing"
      ],
      "metadata": {
        "id": "C_QZyJIfQiMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentence to be tokenized and converted to indices\n",
        "my_tokens = 'he painted the car red'\n",
        "# Apply the text pipeline to the sentence to get token indices\n",
        "input_data = torch.tensor(text_pipeline(my_tokens), dtype=torch.long)\n",
        "\n",
        "# Print out the shape and the token indices tensor\n",
        "print(input_data.shape)\n",
        "print(input_data)"
      ],
      "metadata": {
        "id": "5bGT8qNrO3hI",
        "outputId": "4f5f35f8-0905-498a-f099-44fadb2f3766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n",
            "tensor([12, 13, 15, 11, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the tokenized input data through the embedding layer and the attention mechanism of the Head class\n",
        "embedded_x, k, q, v, w = attention_head.attention(input_data)\n",
        "\n",
        "# Print the size of the resulting embedded vector for verification\n",
        "print(embedded_x.shape)  # Should show the shape as [number of tokens, embedding dimension]\n",
        "print(\"embedded_x:\", embedded_x)  # The actual embedded representations of the input tokens"
      ],
      "metadata": {
        "id": "ofWRKUa2O5MI",
        "outputId": "bf71f678-8418-462d-c835-718543b71f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "embedded_x: tensor([[-1.7371, -0.1153,  0.1805],\n",
            "        [ 1.1282,  0.4349, -0.4054],\n",
            "        [ 0.3940, -0.8697,  0.1545],\n",
            "        [ 1.0329, -0.6806, -1.0346],\n",
            "        [-0.1241, -1.5949, -0.7011]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shapes of the key, query, value, and attention weight matrices\n",
        "# This helps verify the dimensions are as expected for the attention calculations\n",
        "print(\"k:\", k.shape)  # The shape of the keys tensor\n",
        "print(\"q:\", q.shape)  # The shape of the queries tensor\n",
        "print(\"v:\", v.shape)  # The shape of the values tensor\n",
        "print(\"w:\", w.shape)  # The shape of the attention weights tensor"
      ],
      "metadata": {
        "id": "xI9SrNdeREiK",
        "outputId": "6d138602-20c9-4b8f-e8a3-c8364d0904a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: torch.Size([5, 3])\n",
            "q: torch.Size([5, 3])\n",
            "v: torch.Size([5, 3])\n",
            "w: torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we actually pass the input data through the entire attention head to get the output\n",
        "output = attention_head(input_data)\n",
        "\n",
        "# Print the output and its shape, which should match the shape of the input\n",
        "# The output tensor contains the final attention-applied representations of the input\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "QgFj3HnFRbT1",
        "outputId": "ad41b8b4-ed8f-4005-a25a-af1f35b4d2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[-0.4468, -0.3053, -0.2178],\n",
            "        [-0.4902, -0.3217, -0.2156],\n",
            "        [-0.4934, -0.3290, -0.2289],\n",
            "        [-0.5329, -0.3399, -0.2407],\n",
            "        [-0.5340, -0.3435, -0.2534]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional encoding"
      ],
      "metadata": {
        "id": "USR3cIgIRilw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a range of position indices from 0 to the size of the vocabulary\n",
        "position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "# Retrieve the list of words from the vocabulary object\n",
        "vocab_list = list(vocab.get_itos())\n",
        "\n",
        "# Iterate over the range of vocabulary size\n",
        "for idx in range(vocab_size):\n",
        "    word = vocab_list[idx]  # Get the word from the vocabulary list at the current index\n",
        "    pos = position[idx][0].item()  # Extract the numerical value of the position index from the tensor\n",
        "    print(f\"Word: {word}, Position Index: {pos}\")"
      ],
      "metadata": {
        "id": "HnIi2vtQRed8",
        "outputId": "7cb9a9ba-001a-4246-cb8a-312b9e7b7e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: <unk>, Position Index: 0.0\n",
            "Word: nlp, Position Index: 1.0\n",
            "Word: pytorch, Position Index: 2.0\n",
            "Word: analysis, Position Index: 3.0\n",
            "Word: entity, Position Index: 4.0\n",
            "Word: machine, Position Index: 5.0\n",
            "Word: named, Position Index: 6.0\n",
            "Word: sentiment, Position Index: 7.0\n",
            "Word: translation, Position Index: 8.0\n",
            "Word: with, Position Index: 9.0\n",
            "Word: ,, Position Index: 10.0\n",
            "Word: car, Position Index: 11.0\n",
            "Word: he, Position Index: 12.0\n",
            "Word: painted, Position Index: 13.0\n",
            "Word: red, Position Index: 14.0\n",
            "Word: the, Position Index: 15.0\n",
            "Word: basics, Position Index: 16.0\n",
            "Word: classification, Position Index: 17.0\n",
            "Word: for, Position Index: 18.0\n",
            "Word: introduction, Position Index: 19.0\n",
            "Word: of, Position Index: 20.0\n",
            "Word: recognition, Position Index: 21.0\n",
            "Word: techniques, Position Index: 22.0\n",
            "Word: text, Position Index: 23.0\n",
            "Word: to, Position Index: 24.0\n",
            "Word: using, Position Index: 25.0\n",
            "Word: vs, Position Index: 26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a matrix of zeros with dimensions [vocab_size, n_embd]\n",
        "# This will be used to hold the positional encodings for each word in the vocabulary\n",
        "pe = torch.zeros(vocab_size, n_embd)\n",
        "\n",
        "# Concatenate the position tensor three times along dimension 1 (columns)\n",
        "# This simplistic example is likely a placeholder for a more complex function of position that would be used in a real positional encoding\n",
        "pe = torch.cat((position, position, position), 1)\n",
        "\n",
        "# Display the positional encoding tensor\n",
        "# In an actual implementation, this would involve a more sophisticated method that reflects the position's impact on the embeddings\n",
        "pe"
      ],
      "metadata": {
        "id": "jhlTQd8CRnMP",
        "outputId": "eb8698be-ad24-4f6b-ff15-f8aaa6fb267c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  0.,  0.],\n",
              "        [ 1.,  1.,  1.],\n",
              "        [ 2.,  2.,  2.],\n",
              "        [ 3.,  3.,  3.],\n",
              "        [ 4.,  4.,  4.],\n",
              "        [ 5.,  5.,  5.],\n",
              "        [ 6.,  6.,  6.],\n",
              "        [ 7.,  7.,  7.],\n",
              "        [ 8.,  8.,  8.],\n",
              "        [ 9.,  9.,  9.],\n",
              "        [10., 10., 10.],\n",
              "        [11., 11., 11.],\n",
              "        [12., 12., 12.],\n",
              "        [13., 13., 13.],\n",
              "        [14., 14., 14.],\n",
              "        [15., 15., 15.],\n",
              "        [16., 16., 16.],\n",
              "        [17., 17., 17.],\n",
              "        [18., 18., 18.],\n",
              "        [19., 19., 19.],\n",
              "        [20., 20., 20.],\n",
              "        [21., 21., 21.],\n",
              "        [22., 22., 22.],\n",
              "        [23., 23., 23.],\n",
              "        [24., 24., 24.],\n",
              "        [25., 25., 25.],\n",
              "        [26., 26., 26.]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the shape of the embedded_x tensor which contains the embedded representations\n",
        "# 'samples' will hold the number of tokens/samples in the batch\n",
        "# 'dim' will hold the dimensionality of the embeddings\n",
        "samples, dim = embedded_x.shape\n",
        "\n",
        "# Print the tuple (samples, dim) to show the size of the batch and the embedding dimension\n",
        "samples, dim"
      ],
      "metadata": {
        "id": "IedoPCPHRrFs",
        "outputId": "c6d7485e-966d-4554-fee6-b8c8e5c152fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the appropriate positional encodings based on the number of samples and embedding dimensions\n",
        "# This slice of the positional encoding tensor 'pe' corresponds to the actual batch size and embedding size\n",
        "pe_slice = pe[0:samples, 0:n_embd]"
      ],
      "metadata": {
        "id": "ObF0HBM5Rt5N"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CvPjKNqRv8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}