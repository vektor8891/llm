{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/19_lora_pytorch/19_lora_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.17.2\n",
        "# !pip install portalocker==2.8.2\n",
        "# !pip install torchdata==0.7.1"
      ],
      "metadata": {
        "id": "c1MfMelhSHDC"
      },
      "id": "c1MfMelhSHDC",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA with PyTorch\n",
        "\n",
        "- Low-Rank Adaptation (LoRA): fine-tune model by introducing small fraction of additional parameters compared to the total number of parameters in a large model\n",
        "\n",
        "### Defining helper functions"
      ],
      "metadata": {
        "id": "XWAWkTaZQ8RZ"
      },
      "id": "XWAWkTaZQ8RZ"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "91da9c1b",
      "metadata": {
        "id": "91da9c1b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot(COST,ACC):\n",
        "    fig, ax1 = plt.subplots()\n",
        "    color = 'tab:red'\n",
        "    ax1.plot(COST, color=color)\n",
        "    ax1.set_xlabel('epoch', color=color)\n",
        "    ax1.set_ylabel('total loss', color=color)\n",
        "    ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('accuracy', color=color)  # You already handled the x-label with ax1\n",
        "    ax2.plot(ACC, color=color)\n",
        "    ax2.tick_params(axis='y', color=color)\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline"
      ],
      "metadata": {
        "id": "ihKs9T6KSALd"
      },
      "id": "ihKs9T6KSALd"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for  _,text in data_iter:\n",
        "        yield tokenizer(text)"
      ],
      "metadata": {
        "id": "1hMw9h-yR6ce"
      },
      "id": "1hMw9h-yR6ce",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe, Vectors\n",
        "\n",
        "# Note that GloVe embeddings are typically downloaded using:\n",
        "#glove_embedding = GloVe(name=\"6B\", dim=100)\n",
        "# However, the GloVe server is frequently down. The code below offers a workaround\n",
        "\n",
        "class GloVe_override(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "class GloVe_override2(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "try:\n",
        "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
        "except:\n",
        "    try:\n",
        "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
        "    except:\n",
        "        glove_embedding = GloVe(name=\"6B\", dim=100)"
      ],
      "metadata": {
        "id": "38tsuxcEVU0w",
        "outputId": "78a5ca27-3088-444d-ee2b-ff9e8b1ec2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "38tsuxcEVU0w",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove-6B.zip: 134MB [00:01, 85.3MB/s]                          \n",
            "100%|█████████▉| 399999/400000 [00:26<00:00, 15340.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab\n",
        "\n",
        "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "def text_pipeline(x):\n",
        "  return vocab(tokenizer(x))\n",
        "\n",
        "def label_pipeline(x):\n",
        "   return int(x)"
      ],
      "metadata": {
        "id": "GIuoeNQyVmAc"
      },
      "id": "GIuoeNQyVmAc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB dataset"
      ],
      "metadata": {
        "id": "pHCnhvLoV4qL"
      },
      "id": "pHCnhvLoV4qL"
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import tarfile\n",
        "import tempfile\n",
        "import io\n",
        "\n",
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
        "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
        "tempdir = tempfile.TemporaryDirectory()\n",
        "tar.extractall(tempdir.name)\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "L-g1XGkvV4Rq"
      },
      "id": "L-g1XGkvV4Rq",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, root_dir, train=True):\n",
        "        \"\"\"\n",
        "        root_dir: The base directory of the IMDB dataset.\n",
        "        train: A boolean flag indicating whether to use training or test data.\n",
        "        \"\"\"\n",
        "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
        "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
        "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
        "        self.files = self.neg_files + self.pos_files\n",
        "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
        "        self.pos_inx=len(self.pos_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        return label, content"
      ],
      "metadata": {
        "id": "-Rb_dofXV3xx"
      },
      "id": "-Rb_dofXV3xx",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
        "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data\n",
        "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test dataart=train_iter.pos_inx"
      ],
      "metadata": {
        "id": "8QZlcRfTWLuJ"
      },
      "id": "8QZlcRfTWLuJ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duUonSSjWN1n"
      },
      "id": "duUonSSjWN1n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}