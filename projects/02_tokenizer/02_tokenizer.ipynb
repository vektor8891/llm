{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "myFtLwtU0B_A",
        "outputId": "db9c550d-d294-4341-b641-7905dca89779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This showcases word_tokenize from nltk library\n",
        "text = \"This is a sample sentence for word tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "y3I9emOB1bK6",
        "outputId": "5c765b2d-10a8-4e5c-bfd0-57fd0d880b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "4M97f3XG1h_q",
        "outputId": "2e6f9be7-03f1-46aa-e976-7111a533c873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
        "import spacy\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "kF1vxVAm1xjD",
        "outputId": "930e6d81-c86e-419b-f1a2-f2fb24cf30ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
            "I PRON nsubj\n",
            "could AUX aux\n",
            "n't PART neg\n",
            "help VERB ROOT\n",
            "the DET det\n",
            "dog NOUN dobj\n",
            ". PUNCT punct\n",
            "Ca AUX aux\n",
            "n't PART neg\n",
            "you PRON nsubj\n",
            "do VERB ROOT\n",
            "it PRON dobj\n",
            "? PUNCT punct\n",
            "Do AUX aux\n",
            "n't PART neg\n",
            "be AUX ROOT\n",
            "afraid ADJ acomp\n",
            "if SCONJ mark\n",
            "you PRON nsubj\n",
            "are AUX advcl\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem with word-based tokenizer: it treats singular and plural separately\n",
        "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ],
      "metadata": {
        "id": "A3g8moeh2LqK",
        "outputId": "cf864512-c6c3-467f-8283-fa5059eed6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that BertTokenizer treats composite words as separate tokens\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"IBM taught me tokenization.\")"
      ],
      "metadata": {
        "id": "2JIX1Quj2PXr",
        "outputId": "a11fa75d-3521-44d6-b499-7e0e1b02767d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ibm', 'taught', 'me', 'token', '##ization', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using Unigram and SentencePiece\n",
        "from transformers import XLNetTokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer.tokenize(\"IBM taught me tokenization.\")"
      ],
      "metadata": {
        "id": "cgBkEOyQ2r0E",
        "outputId": "167a5356-8d61-4ffc-8fb8-f6fc5cff1011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['笆！BM', '笆》aught', '笆［e', '笆》oken', 'ization', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization with PyTorch\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokenizer(dataset[0][1])"
      ],
      "metadata": {
        "id": "SUm0IFMQ3Ehd",
        "outputId": "d9ad4aff-3d8c-4c22-a083-87d6da49e794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6ed1a9f38a32>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenization with PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Introduction to NLP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jThNaMuP3OZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}