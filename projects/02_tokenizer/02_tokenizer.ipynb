{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/02_tokenizer/02_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "myFtLwtU0B_A",
                "outputId": "0b9ae482-461d-4152-fe54-b9acde456813",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download('punkt_tab')\n",
                "from nltk.tokenize import word_tokenize"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# This showcases word_tokenize from nltk library\n",
                "text = \"This is a sample sentence for word tokenization.\"\n",
                "tokens = word_tokenize(text)\n",
                "print(tokens)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "y3I9emOB1bK6",
                "outputId": "b11c6bb3-15ae-4d8b-f2f9-7e5f95c4014f"
            },
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
                "tokens = word_tokenize(text)\n",
                "print(tokens)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "4M97f3XG1h_q",
                "outputId": "a9149e58-d579-4f8c-eb28-70ca51be495a"
            },
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
                "import spacy\n",
                "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
                "nlp = spacy.load(\"en_core_web_sm\")\n",
                "doc = nlp(text)\n",
                "\n",
                "# Making a list of the tokens and priting the list\n",
                "token_list = [token.text for token in doc]\n",
                "print(\"Tokens:\", token_list)\n",
                "\n",
                "# Showing token details\n",
                "for token in doc:\n",
                "    print(token.text, token.pos_, token.dep_)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "kF1vxVAm1xjD",
                "outputId": "b7fc7be4-9c4b-4f7a-ebf0-1111f1797b35"
            },
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
                        "I PRON nsubj\n",
                        "could AUX aux\n",
                        "n't PART neg\n",
                        "help VERB ROOT\n",
                        "the DET det\n",
                        "dog NOUN dobj\n",
                        ". PUNCT punct\n",
                        "Ca AUX aux\n",
                        "n't PART neg\n",
                        "you PRON nsubj\n",
                        "do VERB ROOT\n",
                        "it PRON dobj\n",
                        "? PUNCT punct\n",
                        "Do AUX aux\n",
                        "n't PART neg\n",
                        "be AUX ROOT\n",
                        "afraid ADJ acomp\n",
                        "if SCONJ mark\n",
                        "you PRON nsubj\n",
                        "are AUX advcl\n",
                        ". PUNCT punct\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Problem with word-based tokenizer: it treats singular and plural separately\n",
                "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
                "token = word_tokenize(text)\n",
                "print(token)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "A3g8moeh2LqK",
                "outputId": "d6eb9dea-e412-4808-b2cc-736e28864c56"
            },
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.']\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Note that BertTokenizer treats composite words as separate tokens\n",
                "from transformers import BertTokenizer\n",
                "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "tokenizer.tokenize(\"IBM taught me tokenization.\")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "2JIX1Quj2PXr",
                "outputId": "5abbbe4a-9378-49f2-89f4-9f05f9e6d149"
            },
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['ibm', 'taught', 'me', 'token', '##ization', '.']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Tokenization using Unigram and SentencePiece\n",
                "from transformers import XLNetTokenizer\n",
                "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "tokenizer.tokenize(\"IBM taught me tokenization.\")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "cgBkEOyQ2r0E",
                "outputId": "e6394886-5cf4-4e8d-9d12-24e6e730fd80"
            },
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['\u2581IBM', '\u2581taught', '\u2581me', '\u2581token', 'ization', '.']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Tokenization with PyTorch\n",
                "from torchtext.data.utils import get_tokenizer\n",
                "dataset = [\n",
                "    (1,\"Introduction to NLP\"),\n",
                "    (2,\"Basics of PyTorch\"),\n",
                "    (1,\"NLP Techniques for Text Classification\"),\n",
                "    (3,\"Named Entity Recognition with PyTorch\"),\n",
                "    (3,\"Sentiment Analysis using PyTorch\"),\n",
                "    (3,\"Machine Translation with PyTorch\"),\n",
                "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
                "    (1,\" Machine Translation with NLP \"),\n",
                "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
                "tokenizer = get_tokenizer(\"basic_english\")\n",
                "tokenizer(dataset[0][1])"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "SUm0IFMQ3Ehd",
                "outputId": "8e5a7d17-5b72-4258-93eb-c3127146dbd7"
            },
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['introduction', 'to', 'nlp']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Token indices\n",
                "def yield_tokens(data_iter):\n",
                "    for  _,text in data_iter:\n",
                "        yield tokenizer(text)\n",
                "my_iterator = yield_tokens(dataset)\n",
                "next(my_iterator)\n"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "jThNaMuP3OZY",
                "outputId": "d4efe820-c2e3-4e6f-c858-7760fbee0909"
            },
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['introduction', 'to', 'nlp']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Out-of-vocabulary (OOV)\n",
                "from torchtext.vocab import build_vocab_from_iterator\n",
                "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
                "vocab.set_default_index(vocab[\"<unk>\"])\n",
                "\n",
                "def get_tokenized_sentence_and_indices(iterator):\n",
                "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
                "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
                "    return tokenized_sentence, token_indices\n",
                "\n",
                "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
                "next(my_iterator)\n",
                "\n",
                "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
                "print(\"Token Indices:\", token_indices)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "a70UPnOz3-Qw",
                "outputId": "aad27887-485b-45c8-e558-1af5837fea26"
            },
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
                        "Token Indices: [11, 15, 2]\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Demonstrate tokenization and the building of vocabulary in PyTorch\n",
                "lines = [\"IBM taught me tokenization\",\n",
                "         \"Special tokenizers are ready and they will blow your mind\",\n",
                "         \"just saying hi!\"]\n",
                "\n",
                "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
                "\n",
                "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
                "\n",
                "tokens = []\n",
                "max_length = 0\n",
                "\n",
                "for line in lines:\n",
                "    tokenized_line = tokenizer_en(line)\n",
                "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
                "    tokens.append(tokenized_line)\n",
                "    max_length = max(max_length, len(tokenized_line))\n",
                "\n",
                "for i in range(len(tokens)):\n",
                "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
                "\n",
                "print(\"Lines after adding special tokens:\\n\", tokens)\n",
                "\n",
                "# Build vocabulary without unk_init\n",
                "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
                "vocab.set_default_index(vocab[\"<unk>\"])\n",
                "\n",
                "# Vocabulary and Token Ids\n",
                "print(\"Vocabulary:\", vocab.get_itos())\n",
                "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "74AIMZ3m7jyJ",
                "outputId": "06185752-2b6f-4fa5-d459-07a74569a7d7"
            },
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Lines after adding special tokens:\n",
                        " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
                        "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
                        "Token IDs for 'tokenization': {'will': 20, 'tokenizers': 19, 'tokenization': 18, 'taught': 16, 'your': 21, 'saying': 15, '<unk>': 0, 'and': 7, 'hi': 10, '<pad>': 1, '<bos>': 2, 'they': 17, '<eos>': 3, '!': 4, 'ready': 14, 'IBM': 5, 'are': 8, 'Special': 6, 'mind': 13, 'me': 12, 'blow': 9, 'just': 11}\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
                "\n",
                "# Tokenize the new line\n",
                "tokenized_new_line = tokenizer_en(new_line)\n",
                "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
                "\n",
                "# Pad the new line to match the maximum length of previous lines\n",
                "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
                "\n",
                "# Convert tokens to IDs and handle unknown words\n",
                "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
                "\n",
                "# Example usage\n",
                "print(\"Token IDs for new line:\", new_line_ids)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "gSRHoxRm796E",
                "outputId": "cc631cdb-0af5-4871-c122-26fc83f83e97"
            },
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Token IDs for new line: [2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 1, 1]\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Comparative text tokenization and performance analysis\n",
                "text = \"\"\"\n",
                "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey\u2019s been enlightening and, honestly, a bunch of fun.\n",
                "Eager to see where this learning path takes me next!\"\n",
                "\"\"\"\n",
                "\n",
                "# Counting and displaying tokens and their frequency\n",
                "from collections import Counter\n",
                "def show_frequencies(tokens, method_name):\n",
                "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
            ],
            "metadata": {
                "id": "3dBGYHOl9Uvk"
            },
            "execution_count": 13,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# NLTK Tokenization\n",
                "from datetime import datetime\n",
                "start_time = datetime.now()\n",
                "nltk_tokens = nltk.word_tokenize(text)\n",
                "nltk_time = datetime.now() - start_time\n",
                "\n",
                "# SpaCy Tokenization\n",
                "nlp = spacy.load(\"en_core_web_sm\")\n",
                "start_time = datetime.now()\n",
                "spacy_tokens = [token.text for token in nlp(text)]\n",
                "spacy_time = datetime.now() - start_time\n",
                "\n",
                "# BertTokenizer Tokenization\n",
                "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "start_time = datetime.now()\n",
                "bert_tokens = bert_tokenizer.tokenize(text)\n",
                "bert_time = datetime.now() - start_time\n",
                "\n",
                "# XLNetTokenizer Tokenization\n",
                "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "start_time = datetime.now()\n",
                "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
                "xlnet_time = datetime.now() - start_time\n",
                "\n",
                "# Display tokens, time taken for each tokenizer, and token frequencies\n",
                "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
                "show_frequencies(nltk_tokens, \"NLTK\")\n",
                "\n",
                "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
                "show_frequencies(spacy_tokens, \"SpaCy\")\n",
                "\n",
                "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
                "show_frequencies(bert_tokens, \"Bert\")\n",
                "\n",
                "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
                "show_frequencies(xlnet_tokens, \"XLNet\")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "YJSqUdzM9YhR",
                "outputId": "e8b0ba2a-7055-4c27-bf95-e1bcdb7c82f0"
            },
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '\u2019', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
                        "Time Taken: 0:00:00.000761 seconds\n",
                        "\n",
                        "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '\u2019': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
                        "\n",
                        "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '\u2019s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
                        "Time Taken: 0:00:00.025483 seconds\n",
                        "\n",
                        "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '\u2019s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
                        "\n",
                        "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '\u2019', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
                        "Time Taken: 0:00:00.001386 seconds\n",
                        "\n",
                        "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '\u2019': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
                        "\n",
                        "XLNet Tokens: ['\u2581Going', '\u2581through', '\u2581the', '\u2581world', '\u2581of', '\u2581token', 'ization', '\u2581has', '\u2581been', '\u2581like', '\u2581walking', '\u2581through', '\u2581a', '\u2581huge', '\u2581maze', '\u2581made', '\u2581of', '\u2581words', ',', '\u2581symbols', ',', '\u2581and', '\u2581meaning', 's', '.', '\u2581Each', '\u2581turn', '\u2581shows', '\u2581a', '\u2581bit', '\u2581more', '\u2581about', '\u2581the', '\u2581cool', '\u2581ways', '\u2581computers', '\u2581learn', '\u2581to', '\u2581understand', '\u2581our', '\u2581language', '.', '\u2581And', '\u2581while', '\u2581I', \"'\", 'm', '\u2581still', '\u2581finding', '\u2581my', '\u2581way', '\u2581through', '\u2581it', ',', '\u2581the', '\u2581journey', '\u2019', 's', '\u2581been', '\u2581enlighten', 'ing', '\u2581and', ',', '\u2581honestly', ',', '\u2581a', '\u2581bunch', '\u2581of', '\u2581fun', '.', '\u2581E', 'ager', '\u2581to', '\u2581see', '\u2581where', '\u2581this', '\u2581learning', '\u2581path', '\u2581takes', '\u2581me', '\u2581next', '!', '\"']\n",
                        "Time Taken: 0:00:00.000685 seconds\n",
                        "\n",
                        "XLNet Token Frequencies: {'\u2581Going': 1, '\u2581through': 3, '\u2581the': 3, '\u2581world': 1, '\u2581of': 3, '\u2581token': 1, 'ization': 1, '\u2581has': 1, '\u2581been': 2, '\u2581like': 1, '\u2581walking': 1, '\u2581a': 3, '\u2581huge': 1, '\u2581maze': 1, '\u2581made': 1, '\u2581words': 1, ',': 5, '\u2581symbols': 1, '\u2581and': 2, '\u2581meaning': 1, 's': 2, '.': 3, '\u2581Each': 1, '\u2581turn': 1, '\u2581shows': 1, '\u2581bit': 1, '\u2581more': 1, '\u2581about': 1, '\u2581cool': 1, '\u2581ways': 1, '\u2581computers': 1, '\u2581learn': 1, '\u2581to': 2, '\u2581understand': 1, '\u2581our': 1, '\u2581language': 1, '\u2581And': 1, '\u2581while': 1, '\u2581I': 1, \"'\": 1, 'm': 1, '\u2581still': 1, '\u2581finding': 1, '\u2581my': 1, '\u2581way': 1, '\u2581it': 1, '\u2581journey': 1, '\u2019': 1, '\u2581enlighten': 1, 'ing': 1, '\u2581honestly': 1, '\u2581bunch': 1, '\u2581fun': 1, '\u2581E': 1, 'ager': 1, '\u2581see': 1, '\u2581where': 1, '\u2581this': 1, '\u2581learning': 1, '\u2581path': 1, '\u2581takes': 1, '\u2581me': 1, '\u2581next': 1, '!': 1, '\"': 1}\n",
                        "\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "!pip freeze > requirements.txt"
            ],
            "metadata": {
                "id": "g2RmecqR9hQG"
            },
            "execution_count": 15,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        },
        "colab": {
            "provenance": [],
            "include_colab_link": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}