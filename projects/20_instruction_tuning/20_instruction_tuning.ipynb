{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/20_instruction_tuning/20_instruction_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -qq datasets==2.20.0\n",
        "!pip install --user -qq evaluate==0.4.2\n",
        "!pip install --user -qq sacrebleu==2.4.2\n",
        "!pip install --user -qq trl==0.9.6"
      ],
      "metadata": {
        "id": "-nugMfWiW5V9"
      },
      "id": "-nugMfWiW5V9",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the device"
      ],
      "metadata": {
        "id": "a76FI60tUY9s"
      },
      "id": "a76FI60tUY9s"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70d9e622",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "70d9e622",
        "outputId": "b3ce49da-4d56-48d3-b933-f08da768dd2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_5, version libnvJitLink.so.12",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-453c22f8e942>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_5, version libnvJitLink.so.12",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset description"
      ],
      "metadata": {
        "id": "Qxodb9G-VrQI"
      },
      "id": "Qxodb9G-VrQI"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json"
      ],
      "metadata": {
        "id": "EgxpSl_2WN2J"
      },
      "id": "EgxpSl_2WN2J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"CodeAlpaca-20k.json\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "Y8E8AIR7VYWL"
      },
      "id": "Y8E8AIR7VYWL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1000]"
      ],
      "metadata": {
        "id": "cMd5z3HsVzwt"
      },
      "id": "cMd5z3HsVzwt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda example: example[\"input\"] == '')"
      ],
      "metadata": {
        "id": "4ZqZAAByXHUi"
      },
      "id": "4ZqZAAByXHUi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=42)"
      ],
      "metadata": {
        "id": "rYkEyj3nXPO_"
      },
      "id": "rYkEyj3nXPO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "aZdz6TOYXRZN"
      },
      "id": "aZdz6TOYXRZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the first 100 rows before splitting\n",
        "dataset_first_100 = dataset.select(range(100))\n",
        "\n",
        "dataset_split = dataset_first_100.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset_split['train']\n",
        "test_dataset = dataset_split['test']\n",
        "dataset_first_100"
      ],
      "metadata": {
        "id": "GoO9WtJTXUSH"
      },
      "id": "GoO9WtJTXUSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a small set of data for the resource limitation\n",
        "# This dataset will be only used for evaluation parts, not for the training\n",
        "tiny_test_dataset=test_dataset.select(range(10))\n",
        "tiny_train_dataset=train_dataset.select(range(10))"
      ],
      "metadata": {
        "id": "70lQ7z_PXWDu"
      },
      "id": "70lQ7z_PXWDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and tokenizer"
      ],
      "metadata": {
        "id": "EXcNF4aPXz11"
      },
      "id": "EXcNF4aPXz11"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
      ],
      "metadata": {
        "id": "s_Mq25gHXyV0"
      },
      "id": "s_Mq25gHXyV0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')"
      ],
      "metadata": {
        "id": "GzMb3arbX5gF"
      },
      "id": "GzMb3arbX5gF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "QPPnFhUyYFDz"
      },
      "id": "QPPnFhUyYFDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "-TWoWWcyYNvl"
      },
      "id": "-TWoWWcyYNvl"
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n{mydataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_no_response(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ],
      "metadata": {
        "id": "tf_Ytrp_YNJp"
      },
      "id": "tf_Ytrp_YNJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "expected_outputs = []\n",
        "instructions_with_responses = formatting_prompts_func(test_dataset)\n",
        "instructions = formatting_prompts_func_no_response(test_dataset)\n",
        "for i in tqdm(range(len(instructions_with_responses))):\n",
        "    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors=\"pt\", max_length=1024, truncation=True, padding=False)\n",
        "    tokenized_instruction = tokenizer(instructions[i], return_tensors=\"pt\")\n",
        "    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)\n",
        "    expected_outputs.append(expected_output)"
      ],
      "metadata": {
        "id": "-qbpaOVwYGfk"
      },
      "id": "-qbpaOVwYGfk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('############## instructions ##############\\n' + instructions[0])\n",
        "print('############## instructions_with_responses ##############\\n' + instructions_with_responses[0])\n",
        "print('\\n############## expected_outputs ##############' + expected_outputs[0])"
      ],
      "metadata": {
        "id": "j7W9QTvbYhh7"
      },
      "id": "j7W9QTvbYhh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, original_list):\n",
        "        self.original_list = original_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_list)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.original_list[i]\n",
        "\n",
        "instructions_torch = ListDataset(instructions)"
      ],
      "metadata": {
        "id": "W3QY9ixAYp28"
      },
      "id": "W3QY9ixAYp28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions_torch[0]"
      ],
      "metadata": {
        "id": "NrXnjbmUYx1f"
      },
      "id": "NrXnjbmUYx1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the base model"
      ],
      "metadata": {
        "id": "3DwoaTHIY2y5"
      },
      "id": "3DwoaTHIY2y5"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen_pipeline = pipeline(\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device,\n",
        "                        batch_size=2,\n",
        "                        max_length=50,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_full_text=False)"
      ],
      "metadata": {
        "id": "klrR0vIyY2Wv"
      },
      "id": "klrR0vIyY2Wv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
        "    pipeline_iterator= gen_pipeline(instructions_torch[:3],\n",
        "                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
        "                                    num_beams=5,\n",
        "                                    early_stopping=True,)\n",
        "\n",
        "generated_outputs_base = []\n",
        "for text in pipeline_iterator:\n",
        "    generated_outputs_base.append(text[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "hjRyIDdUYzMc"
      },
      "id": "hjRyIDdUYzMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# load generated responses from CUDA-enabled GPU machine\n",
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VvQRrSqS1P0_GobqtL-SKA/instruction-tuning-generated-outputs-base.pkl')\n",
        "generated_outputs_base = pickle.load(io.BytesIO(urlopened.read()))"
      ],
      "metadata": {
        "id": "uusogqGIZ4X7"
      },
      "id": "uusogqGIZ4X7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print('@@@@@@@@@@@@@@@@@@@@')\n",
        "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
        "    print(instructions[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
        "    print(expected_outputs[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
        "    print(generated_outputs_base[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@@@@@@@@@@@@@@@@')"
      ],
      "metadata": {
        "id": "FMF6Kc4LaFQo"
      },
      "id": "FMF6Kc4LaFQo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issues of base model responses:\n",
        "\n",
        "1. responses not up to mark\n",
        "1. tendency to eextend and repeat the answers\n",
        "\n",
        "-> Instruction-tuning can fix both issues!"
      ],
      "metadata": {
        "id": "TlBeWnv6apYq"
      },
      "id": "TlBeWnv6apYq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU score"
      ],
      "metadata": {
        "id": "MBgbR4Vxaiy1"
      },
      "id": "MBgbR4Vxaiy1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade evaluate"
      ],
      "metadata": {
        "id": "g5UIjReJiFiq"
      },
      "id": "g5UIjReJiFiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sacrebleu\n",
        "# sacrebleu.corpus_bleu(generated_outputs_base, expected_outputs)\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "results_base = sacrebleu.compute(predictions=generated_outputs_base,\n",
        "                                 references=expected_outputs)\n",
        "\n",
        "print(list(results_base.keys()))\n",
        "print(round(results_base[\"score\"], 1))"
      ],
      "metadata": {
        "id": "kTY5Nq2qaNXS"
      },
      "id": "kTY5Nq2qaNXS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform instruction fine-tuning with LoRA"
      ],
      "metadata": {
        "id": "FNaxSVsmbq5K"
      },
      "id": "FNaxSVsmbq5K"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Low-rank dimension\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "D9PySlHObG2Z"
      },
      "id": "D9PySlHObG2Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template = \"### Response:\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "JnX1F7ExdgoT"
      },
      "id": "JnX1F7ExdgoT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0vvn_I_gzLr"
      },
      "id": "T0vvn_I_gzLr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}