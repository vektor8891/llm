{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vektor8891/llm/blob/main/projects/20_instruction_tuning/20_instruction_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --user -qq datasets==2.20.0\n",
        "# !pip install --user -qq evaluate==0.4.2\n",
        "# !pip install --user -qq sacrebleu==2.4.2"
      ],
      "metadata": {
        "id": "-nugMfWiW5V9"
      },
      "id": "-nugMfWiW5V9",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the device"
      ],
      "metadata": {
        "id": "a76FI60tUY9s"
      },
      "id": "a76FI60tUY9s"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70d9e622",
      "metadata": {
        "id": "70d9e622"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset description"
      ],
      "metadata": {
        "id": "Qxodb9G-VrQI"
      },
      "id": "Qxodb9G-VrQI"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json"
      ],
      "metadata": {
        "id": "EgxpSl_2WN2J",
        "outputId": "9a9206f8-25fd-42a3-ef74-b79c4d06b162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EgxpSl_2WN2J",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-02 21:02:45--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6957007 (6.6M) [application/json]\n",
            "Saving to: ‘CodeAlpaca-20k.json.4’\n",
            "\n",
            "CodeAlpaca-20k.json 100%[===================>]   6.63M  9.49MB/s    in 0.7s    \n",
            "\n",
            "2025-06-02 21:02:47 (9.49 MB/s) - ‘CodeAlpaca-20k.json.4’ saved [6957007/6957007]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"CodeAlpaca-20k.json\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "Y8E8AIR7VYWL",
        "outputId": "4d96e20c-8700-416e-9a7e-815d76c7f996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Y8E8AIR7VYWL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input'],\n",
              "    num_rows: 20022\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1000]"
      ],
      "metadata": {
        "id": "cMd5z3HsVzwt",
        "outputId": "6ee1d864-eee7-4585-c92a-5dac7c1417e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cMd5z3HsVzwt",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 's = \"Hello world\" \\ns = s[::-1] \\nprint(s)',\n",
              " 'instruction': 'Reverse the string given in the input',\n",
              " 'input': 'Hello world'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda example: example[\"input\"] == '')"
      ],
      "metadata": {
        "id": "4ZqZAAByXHUi"
      },
      "id": "4ZqZAAByXHUi",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=42)"
      ],
      "metadata": {
        "id": "rYkEyj3nXPO_"
      },
      "id": "rYkEyj3nXPO_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "aZdz6TOYXRZN",
        "outputId": "f4e84888-d446-4e30-cc4c-676fadc94d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aZdz6TOYXRZN",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input'],\n",
              "    num_rows: 9764\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the first 100 rows before splitting\n",
        "dataset_first_100 = dataset.select(range(100))\n",
        "\n",
        "dataset_split = dataset_first_100.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset_split['train']\n",
        "test_dataset = dataset_split['test']\n",
        "dataset_first_100"
      ],
      "metadata": {
        "id": "GoO9WtJTXUSH",
        "outputId": "59f95fec-71f5-4b76-a293-9d9a20c015d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GoO9WtJTXUSH",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a small set of data for the resource limitation\n",
        "# This dataset will be only used for evaluation parts, not for the training\n",
        "tiny_test_dataset=test_dataset.select(range(10))\n",
        "tiny_train_dataset=train_dataset.select(range(10))"
      ],
      "metadata": {
        "id": "70lQ7z_PXWDu"
      },
      "id": "70lQ7z_PXWDu",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and tokenizer"
      ],
      "metadata": {
        "id": "EXcNF4aPXz11"
      },
      "id": "EXcNF4aPXz11"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
      ],
      "metadata": {
        "id": "s_Mq25gHXyV0"
      },
      "id": "s_Mq25gHXyV0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')"
      ],
      "metadata": {
        "id": "GzMb3arbX5gF"
      },
      "id": "GzMb3arbX5gF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "QPPnFhUyYFDz",
        "outputId": "008f183b-549c-49a9-e6f6-930c73583165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "QPPnFhUyYFDz",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "-TWoWWcyYNvl"
      },
      "id": "-TWoWWcyYNvl"
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n{mydataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_no_response(mydataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(mydataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ],
      "metadata": {
        "id": "tf_Ytrp_YNJp"
      },
      "id": "tf_Ytrp_YNJp",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "expected_outputs = []\n",
        "instructions_with_responses = formatting_prompts_func(test_dataset)\n",
        "instructions = formatting_prompts_func_no_response(test_dataset)\n",
        "for i in tqdm(range(len(instructions_with_responses))):\n",
        "    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors=\"pt\", max_length=1024, truncation=True, padding=False)\n",
        "    tokenized_instruction = tokenizer(instructions[i], return_tensors=\"pt\")\n",
        "    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)\n",
        "    expected_outputs.append(expected_output)"
      ],
      "metadata": {
        "id": "-qbpaOVwYGfk",
        "outputId": "6631afb0-b5c2-4601-f614-dddb8a022fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-qbpaOVwYGfk",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 605.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('############## instructions ##############\\n' + instructions[0])\n",
        "print('############## instructions_with_responses ##############\\n' + instructions_with_responses[0])\n",
        "print('\\n############## expected_outputs ##############' + expected_outputs[0])"
      ],
      "metadata": {
        "id": "j7W9QTvbYhh7",
        "outputId": "cf893d24-aa74-4810-e87c-e599830acbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j7W9QTvbYhh7",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############## instructions ##############\n",
            "### Instruction:\n",
            "Design a regular expression to match strings that begin with the letter \"a\".\n",
            "\n",
            "### Response:\n",
            "\n",
            "############## instructions_with_responses ##############\n",
            "### Instruction:\n",
            "Design a regular expression to match strings that begin with the letter \"a\".\n",
            "\n",
            "### Response:\n",
            "^a.*</s>\n",
            "\n",
            "############## expected_outputs ##############\n",
            "^a.*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, original_list):\n",
        "        self.original_list = original_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_list)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.original_list[i]\n",
        "\n",
        "instructions_torch = ListDataset(instructions)"
      ],
      "metadata": {
        "id": "W3QY9ixAYp28"
      },
      "id": "W3QY9ixAYp28",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions_torch[0]"
      ],
      "metadata": {
        "id": "NrXnjbmUYx1f",
        "outputId": "81f62fde-74c6-4969-ec1f-889dc22d35ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "NrXnjbmUYx1f",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Instruction:\\nDesign a regular expression to match strings that begin with the letter \"a\".\\n\\n### Response:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the base model"
      ],
      "metadata": {
        "id": "3DwoaTHIY2y5"
      },
      "id": "3DwoaTHIY2y5"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen_pipeline = pipeline(\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device,\n",
        "                        batch_size=2,\n",
        "                        max_length=50,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_full_text=False)"
      ],
      "metadata": {
        "id": "klrR0vIyY2Wv",
        "outputId": "59feddf6-8ba9-4576-ee68-9210b37ca3d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "klrR0vIyY2Wv",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
        "    pipeline_iterator= gen_pipeline(instructions_torch[:3],\n",
        "                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
        "                                    num_beams=5,\n",
        "                                    early_stopping=True,)\n",
        "\n",
        "generated_outputs_base = []\n",
        "for text in pipeline_iterator:\n",
        "    generated_outputs_base.append(text[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "hjRyIDdUYzMc"
      },
      "id": "hjRyIDdUYzMc",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# load generated responses from CUDA-enabled GPU machine\n",
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VvQRrSqS1P0_GobqtL-SKA/instruction-tuning-generated-outputs-base.pkl')\n",
        "generated_outputs_base = pickle.load(io.BytesIO(urlopened.read()))"
      ],
      "metadata": {
        "id": "uusogqGIZ4X7"
      },
      "id": "uusogqGIZ4X7",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print('@@@@@@@@@@@@@@@@@@@@')\n",
        "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
        "    print(instructions[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
        "    print(expected_outputs[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
        "    print(generated_outputs_base[i])\n",
        "    print('\\n\\n')\n",
        "    print('@@@@@@@@@@@@@@@@@@@@')"
      ],
      "metadata": {
        "id": "FMF6Kc4LaFQo",
        "outputId": "84534ad0-f0f6-4778-bc8b-7825d507ed10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FMF6Kc4LaFQo",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@ Instruction 1: \n",
            "### Instruction:\n",
            "Design a regular expression to match strings that begin with the letter \"a\".\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Expected response 1: \n",
            "\n",
            "^a.*\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Generated response 1: \n",
            "Thank you for your question.\n",
            "\n",
            "### Response:\n",
            "Thank you for your question.\n",
            "\n",
            "### Response:\n",
            "Thank you for your question.\n",
            "\n",
            "### Response:\n",
            "Thank you for your question.\n",
            "\n",
            "### Response:\n",
            "Thank you for your question.\n",
            "\n",
            "### Response:\n",
            "Thank you for your question.\n",
            "\n",
            "\n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@ Instruction 2: \n",
            "### Instruction:\n",
            "Suggest a solution for validating a password which should contain at least 1 uppercase character, 1 lowercase character and 1 digit.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Expected response 2: \n",
            "\n",
            "We can use a regular expression to validate a password. A regular expression to validate a password with at least 1 uppercase character, 1 lowercase character and 1 digit should be:\n",
            "\n",
            "^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d).+$\n",
            "\n",
            "If the password matches the regular expression, the password is valid. Otherwise, the password is invalid.\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Generated response 2: \n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:\n",
            "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
            "\n",
            "\n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@ Instruction 3: \n",
            "### Instruction:\n",
            "Create an algorithm to check if a binary tree is univalued.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Expected response 3: \n",
            "\n",
            "\"\"\"\n",
            "def isUnivalTree(root):\n",
            "    is_uni = True\n",
            "    if root is None:\n",
            "        return is_uni\n",
            "    else:\n",
            "        val = root.val\n",
            "        if root.left:\n",
            "            is_uni &= root.left.val == val\n",
            "        if root.right:\n",
            "            is_uni &= root.right.val == val\n",
            "        is_uni &= isUnivalTree(root.left)\n",
            "        is_uni &= isUnivalTree(root.right)\n",
            "    return is_uni\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "@@@@@ Generated response 3: \n",
            "The CSS rule is set to “big-header”.\n",
            "\n",
            "### Response:\n",
            "The CSS rule is set to “big-header”.\n",
            "\n",
            "### Response:\n",
            "The CSS rule is set to “big-header”.\n",
            "\n",
            "### Response:\n",
            "The CSS rule is set to “big-header”.\n",
            "\n",
            "### Response:\n",
            "The CSS rule is set to “big-header”.\n",
            "\n",
            "\n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@@\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issues of base model responses:\n",
        "\n",
        "1. responses not up to mark\n",
        "1. tendency to eextend and repeat the answers\n",
        "\n",
        "-> Instruction-tuning can fix both issues!"
      ],
      "metadata": {
        "id": "TlBeWnv6apYq"
      },
      "id": "TlBeWnv6apYq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU score"
      ],
      "metadata": {
        "id": "MBgbR4Vxaiy1"
      },
      "id": "MBgbR4Vxaiy1"
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "results_base = sacrebleu.compute(predictions=generated_outputs_base,\n",
        "                                 references=expected_outputs)\n",
        "\n",
        "print(list(results_base.keys()))\n",
        "print(round(results_base[\"score\"], 1))"
      ],
      "metadata": {
        "id": "kTY5Nq2qaNXS",
        "outputId": "85f706a0-9bfe-42e8-c164-7c5589cbe3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "id": "kTY5Nq2qaNXS",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find a module script at /content/sacrebleu/sacrebleu.py. Module 'sacrebleu' doesn't exist on the Hugging Face Hub either.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b9c2d57d05ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msacrebleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msacrebleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sacrebleu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m results_base = sacrebleu.compute(predictions=generated_outputs_base,\n\u001b[1;32m      5\u001b[0m                                  references=expected_outputs)\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \"\"\"\n\u001b[1;32m    747\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     evaluation_module = evaluation_module_factory(\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     )\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    682\u001b[0m                 \u001b[0;34mf\"Couldn't find a module script at {relative_to_absolute_path(combined_path)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 \u001b[0;34mf\"Module '{path}' doesn't exist on the Hugging Face Hub either.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a module script at /content/sacrebleu/sacrebleu.py. Module 'sacrebleu' doesn't exist on the Hugging Face Hub either."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show sacrebleu"
      ],
      "metadata": {
        "id": "65WoLwu1eCpI",
        "outputId": "aa06eb42-60fb-4b99-953a-beea0ab84337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "65WoLwu1eCpI",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: sacrebleu\n",
            "Version: 2.4.2\n",
            "Summary: Hassle-free computation of shareable, comparable, and reproducible BLEU, chrF, and TER scores\n",
            "Home-page: https://github.com/mjpost/sacrebleu\n",
            "Author: Matt Post\n",
            "Author-email: post@cs.jhu.edu\n",
            "License: Apache License 2.0\n",
            "Location: /root/.local/lib/python3.11/site-packages\n",
            "Requires: colorama, lxml, numpy, portalocker, regex, tabulate\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform instruction fine-tuning with LoRA"
      ],
      "metadata": {
        "id": "FNaxSVsmbq5K"
      },
      "id": "FNaxSVsmbq5K"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Low-rank dimension\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "D9PySlHObG2Z"
      },
      "id": "D9PySlHObG2Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.cache/huggingface/evaluate"
      ],
      "metadata": {
        "id": "HlkOZcoMch_f"
      },
      "id": "HlkOZcoMch_f",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnX1F7ExdgoT"
      },
      "id": "JnX1F7ExdgoT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}